"""é«˜çº§ç”¨æ³•ç¤ºä¾‹

æ¼”ç¤ºAgentlyFormatçš„é«˜çº§åŠŸèƒ½å’Œå¤æ‚ä½¿ç”¨åœºæ™¯ã€‚
"""

import asyncio
import json
import time
from typing import Dict, Any, List, Optional, Callable
from dataclasses import dataclass

from agently_format.core.streaming_parser import StreamingParser
from agently_format.core.json_completer import JSONCompleter
from agently_format.core.path_builder import PathBuilder
from agently_format.core.event_system import EventEmitter
from agently_format.types import (
    ParseEvent, ParseEventType, CompletionStrategy,
    PathStyle, ModelConfig, ChatMessage
)
from agently_format.adapters.factory import ModelAdapterFactory


@dataclass
class ProcessingMetrics:
    """å¤„ç†æŒ‡æ ‡"""
    total_chunks: int = 0
    processed_chunks: int = 0
    total_events: int = 0
    processing_time: float = 0.0
    errors: List[str] = None
    
    def __post_init__(self):
        if self.errors is None:
            self.errors = []


class AdvancedJSONProcessor:
    """é«˜çº§JSONå¤„ç†å™¨"""
    
    def __init__(self):
        self.event_emitter = EventEmitter()
        self.streaming_parser = StreamingParser(self.event_emitter)
        self.json_completer = JSONCompleter()
        self.path_builder = PathBuilder()
        self.model_factory = ModelAdapterFactory()
        
        # å¤„ç†æŒ‡æ ‡
        self.metrics = ProcessingMetrics()
        
        # äº‹ä»¶ç›‘å¬å™¨
        self._setup_event_listeners()
        
        # æ•°æ®ç¼“å­˜
        self.data_cache: Dict[str, Any] = {}
        self.path_cache: Dict[str, List[str]] = {}
    
    def _setup_event_listeners(self):
        """è®¾ç½®äº‹ä»¶ç›‘å¬å™¨"""
        self.event_emitter.on('parse_progress', self._on_parse_progress)
        self.event_emitter.on('parse_error', self._on_parse_error)
        self.event_emitter.on('parse_complete', self._on_parse_complete)
    
    def _on_parse_progress(self, event: ParseEvent):
        """è§£æè¿›åº¦äº‹ä»¶å¤„ç†"""
        self.metrics.total_events += 1
    
    def _on_parse_error(self, event: ParseEvent):
        """è§£æé”™è¯¯äº‹ä»¶å¤„ç†"""
        error_msg = event.data.get('error', 'Unknown error')
        self.metrics.errors.append(error_msg)
    
    def _on_parse_complete(self, event: ParseEvent):
        """è§£æå®Œæˆäº‹ä»¶å¤„ç†"""
        session_id = event.session_id
        data = self.streaming_parser.get_current_data(session_id)
        if data:
            self.data_cache[session_id] = data
    
    async def process_with_ai_completion(
        self,
        incomplete_json: str,
        model_config: Optional[ModelConfig] = None,
        custom_prompt: Optional[str] = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨AIæ¨¡å‹è¿›è¡Œæ™ºèƒ½JSONè¡¥å…¨"""
        print("ğŸ¤– ä½¿ç”¨AIæ¨¡å‹è¿›è¡Œæ™ºèƒ½JSONè¡¥å…¨...")
        
        # é¦–å…ˆå°è¯•ä¼ ç»Ÿè¡¥å…¨
        traditional_result = await self.json_completer.complete(
            incomplete_json,
            strategy=CompletionStrategy.SMART
        )
        
        if traditional_result.is_valid and traditional_result.confidence > 0.8:
            print("âœ… ä¼ ç»Ÿè¡¥å…¨æˆåŠŸï¼Œç½®ä¿¡åº¦é«˜")
            return {
                "method": "traditional",
                "result": traditional_result.completed_json,
                "confidence": traditional_result.confidence
            }
        
        # å¦‚æœä¼ ç»Ÿè¡¥å…¨å¤±è´¥æˆ–ç½®ä¿¡åº¦ä½ï¼Œä½¿ç”¨AIæ¨¡å‹
        if model_config:
            try:
                adapter = self.model_factory.create_adapter(model_config)
                
                # æ„å»ºAIè¡¥å…¨æç¤º
                if custom_prompt:
                    prompt = custom_prompt
                else:
                    prompt = f"""
è¯·å¸®åŠ©è¡¥å…¨ä»¥ä¸‹ä¸å®Œæ•´çš„JSONæ•°æ®ã€‚è¯·ç¡®ä¿ï¼š
1. ä¿æŒåŸæœ‰æ•°æ®ç»“æ„å’Œå€¼ä¸å˜
2. è¡¥å…¨ç¼ºå¤±çš„æ‹¬å·ã€å¼•å·å’Œé€—å·
3. ç¡®ä¿æœ€ç»ˆç»“æœæ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼
4. åªè¿”å›è¡¥å…¨åçš„JSONï¼Œä¸è¦æ·»åŠ å…¶ä»–è¯´æ˜

ä¸å®Œæ•´çš„JSON:
{incomplete_json}

è¡¥å…¨åçš„JSON:
"""
                
                messages = [ChatMessage(role="user", content=prompt)]
                
                response = await adapter.chat_completion(
                    messages=messages,
                    stream=False,
                    temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
                    max_tokens=2000
                )
                
                # æå–JSONå†…å®¹
                ai_completed = response.content.strip()
                
                # å°è¯•è§£æAIè¡¥å…¨çš„ç»“æœ
                try:
                    json.loads(ai_completed)
                    print("âœ… AIè¡¥å…¨æˆåŠŸ")
                    return {
                        "method": "ai",
                        "result": ai_completed,
                        "confidence": 0.9,
                        "model": response.model
                    }
                except json.JSONDecodeError:
                    print("âŒ AIè¡¥å…¨ç»“æœæ— æ•ˆ")
                
                await adapter.close()
                
            except Exception as e:
                print(f"âŒ AIè¡¥å…¨å¤±è´¥: {e}")
        
        # å›é€€åˆ°ä¼ ç»Ÿè¡¥å…¨
        print("ğŸ”„ å›é€€åˆ°ä¼ ç»Ÿè¡¥å…¨")
        return {
            "method": "fallback",
            "result": traditional_result.completed_json,
            "confidence": traditional_result.confidence
        }
    
    async def intelligent_path_analysis(
        self,
        data: Dict[str, Any],
        analysis_type: str = "comprehensive"
    ) -> Dict[str, Any]:
        """æ™ºèƒ½è·¯å¾„åˆ†æ"""
        print(f"ğŸ” æ‰§è¡Œæ™ºèƒ½è·¯å¾„åˆ†æ ({analysis_type})...")
        
        # æ„å»ºæ‰€æœ‰è·¯å¾„
        all_paths = self.path_builder.build_paths(
            data,
            style=PathStyle.DOT,
            include_arrays=True
        )
        
        analysis_result = {
            "total_paths": len(all_paths),
            "analysis_type": analysis_type,
            "statistics": {},
            "recommendations": [],
            "patterns": []
        }
        
        if analysis_type == "comprehensive":
            # æ·±åº¦åˆ†æ
            analysis_result.update(await self._comprehensive_path_analysis(data, all_paths))
        elif analysis_type == "performance":
            # æ€§èƒ½åˆ†æ
            analysis_result.update(await self._performance_path_analysis(data, all_paths))
        elif analysis_type == "structure":
            # ç»“æ„åˆ†æ
            analysis_result.update(await self._structure_path_analysis(data, all_paths))
        
        return analysis_result
    
    async def _comprehensive_path_analysis(
        self,
        data: Dict[str, Any],
        paths: List[str]
    ) -> Dict[str, Any]:
        """ç»¼åˆè·¯å¾„åˆ†æ"""
        # ç»Ÿè®¡ä¸åŒç±»å‹çš„è·¯å¾„
        object_paths = [p for p in paths if not '[' in p]
        array_paths = [p for p in paths if '[' in p]
        
        # åˆ†æåµŒå¥—æ·±åº¦
        max_depth = max(len(p.split('.')) for p in paths) if paths else 0
        avg_depth = sum(len(p.split('.')) for p in paths) / len(paths) if paths else 0
        
        # åˆ†ææ•°æ®ç±»å‹åˆ†å¸ƒ
        type_distribution = {}
        for path in paths:
            try:
                value = self.path_builder.get_value_by_path(data, path)
                value_type = type(value).__name__
                type_distribution[value_type] = type_distribution.get(value_type, 0) + 1
            except:
                continue
        
        return {
            "statistics": {
                "object_paths": len(object_paths),
                "array_paths": len(array_paths),
                "max_depth": max_depth,
                "average_depth": round(avg_depth, 2),
                "type_distribution": type_distribution
            },
            "recommendations": [
                "è€ƒè™‘ä½¿ç”¨ç´¢å¼•ä¼˜åŒ–æ·±å±‚åµŒå¥—è®¿é—®" if max_depth > 5 else "ç»“æ„æ·±åº¦åˆç†",
                "å»ºè®®ç¼“å­˜é¢‘ç¹è®¿é—®çš„è·¯å¾„" if len(paths) > 100 else "è·¯å¾„æ•°é‡é€‚ä¸­",
                "è€ƒè™‘æ•°æ®æ‰å¹³åŒ–" if len(array_paths) > len(object_paths) else "å¯¹è±¡ç»“æ„è‰¯å¥½"
            ]
        }
    
    async def _performance_path_analysis(
        self,
        data: Dict[str, Any],
        paths: List[str]
    ) -> Dict[str, Any]:
        """æ€§èƒ½è·¯å¾„åˆ†æ"""
        # æµ‹è¯•è·¯å¾„è®¿é—®æ€§èƒ½
        start_time = time.time()
        
        access_times = []
        for path in paths[:50]:  # æµ‹è¯•å‰50ä¸ªè·¯å¾„
            path_start = time.time()
            try:
                self.path_builder.get_value_by_path(data, path)
                access_times.append(time.time() - path_start)
            except:
                continue
        
        total_time = time.time() - start_time
        
        return {
            "statistics": {
                "total_access_time": round(total_time, 4),
                "average_access_time": round(sum(access_times) / len(access_times), 6) if access_times else 0,
                "fastest_access": round(min(access_times), 6) if access_times else 0,
                "slowest_access": round(max(access_times), 6) if access_times else 0
            },
            "recommendations": [
                "æ€§èƒ½è‰¯å¥½" if total_time < 0.1 else "è€ƒè™‘ä¼˜åŒ–æ•°æ®ç»“æ„",
                "è®¿é—®é€Ÿåº¦å‡åŒ€" if len(set(round(t, 4) for t in access_times)) < 5 else "å­˜åœ¨æ€§èƒ½ç“¶é¢ˆ"
            ]
        }
    
    async def _structure_path_analysis(
        self,
        data: Dict[str, Any],
        paths: List[str]
    ) -> Dict[str, Any]:
        """ç»“æ„è·¯å¾„åˆ†æ"""
        # åˆ†æè·¯å¾„æ¨¡å¼
        patterns = {}
        for path in paths:
            parts = path.split('.')
            if len(parts) >= 2:
                pattern = '.'.join(parts[:2])  # å–å‰ä¸¤çº§ä½œä¸ºæ¨¡å¼
                patterns[pattern] = patterns.get(pattern, 0) + 1
        
        # æ‰¾å‡ºæœ€å¸¸è§çš„æ¨¡å¼
        common_patterns = sorted(patterns.items(), key=lambda x: x[1], reverse=True)[:5]
        
        return {
            "patterns": [f"{pattern} (å‡ºç°{count}æ¬¡)" for pattern, count in common_patterns],
            "statistics": {
                "unique_patterns": len(patterns),
                "most_common_pattern": common_patterns[0][0] if common_patterns else None
            },
            "recommendations": [
                "ç»“æ„æ¨¡å¼æ¸…æ™°" if len(patterns) < 10 else "è€ƒè™‘é‡æ„å¤æ‚ç»“æ„",
                "æ•°æ®ç»„ç»‡è‰¯å¥½" if common_patterns and common_patterns[0][1] > 5 else "ç»“æ„è¾ƒä¸ºåˆ†æ•£"
            ]
        }
    
    async def adaptive_streaming_processing(
        self,
        data_source: Callable[[], List[str]],
        session_id: str,
        adaptive_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """è‡ªé€‚åº”æµå¼å¤„ç†"""
        print(f"ğŸ”„ å¼€å§‹è‡ªé€‚åº”æµå¼å¤„ç† (ä¼šè¯: {session_id})...")
        
        # åˆ›å»ºè§£æä¼šè¯
        self.streaming_parser.create_session(session_id)
        
        config = adaptive_config or {
            "chunk_size_adjustment": True,
            "error_recovery": True,
            "performance_monitoring": True,
            "dynamic_buffering": True
        }
        
        chunks = data_source()
        self.metrics.total_chunks = len(chunks)
        
        start_time = time.time()
        buffer = ""
        chunk_sizes = []
        processing_times = []
        
        for i, chunk in enumerate(chunks):
            chunk_start = time.time()
            
            # åŠ¨æ€ç¼“å†²
            if config.get("dynamic_buffering"):
                buffer += chunk
                
                # æ ¹æ®æ€§èƒ½è°ƒæ•´å¤„ç†ç­–ç•¥
                if len(processing_times) > 5:
                    avg_time = sum(processing_times[-5:]) / 5
                    if avg_time > 0.1:  # å¦‚æœå¤„ç†æ—¶é—´è¿‡é•¿
                        # å¢åŠ ç¼“å†²åŒºå¤§å°ï¼Œå‡å°‘å¤„ç†é¢‘ç‡
                        if len(buffer) < 1000:
                            continue
                
                process_chunk = buffer
                buffer = ""
            else:
                process_chunk = chunk
            
            try:
                # å¤„ç†å—
                events = await self.streaming_parser.parse_chunk(
                    chunk=process_chunk,
                    session_id=session_id,
                    is_final=(i == len(chunks) - 1)
                )
                
                self.metrics.processed_chunks += 1
                
                # è®°å½•æ€§èƒ½æŒ‡æ ‡
                chunk_time = time.time() - chunk_start
                processing_times.append(chunk_time)
                chunk_sizes.append(len(process_chunk))
                
                # è‡ªé€‚åº”è°ƒæ•´
                if config.get("chunk_size_adjustment") and len(processing_times) > 3:
                    # æ ¹æ®å¤„ç†æ—¶é—´è°ƒæ•´ç­–ç•¥
                    recent_avg = sum(processing_times[-3:]) / 3
                    if recent_avg > 0.2:  # å¤„ç†æ—¶é—´è¿‡é•¿
                        print(f"âš¡ æ£€æµ‹åˆ°æ€§èƒ½ä¸‹é™ï¼Œè°ƒæ•´å¤„ç†ç­–ç•¥")
                
                # é”™è¯¯æ¢å¤
                state = self.streaming_parser.get_session_state(session_id)
                if state and state.errors and config.get("error_recovery"):
                    print(f"ğŸ”§ æ£€æµ‹åˆ°é”™è¯¯ï¼Œå°è¯•æ¢å¤: {state.errors}")
                    # å¯ä»¥åœ¨è¿™é‡Œå®ç°é”™è¯¯æ¢å¤é€»è¾‘
                
            except Exception as e:
                if config.get("error_recovery"):
                    print(f"ğŸš¨ å¤„ç†å¼‚å¸¸ï¼Œè·³è¿‡å½“å‰å—: {e}")
                    self.metrics.errors.append(str(e))
                    continue
                else:
                    raise
        
        # å¤„ç†å‰©ä½™ç¼“å†²åŒº
        if buffer and config.get("dynamic_buffering"):
            events = await self.streaming_parser.parse_chunk(
                chunk=buffer,
                session_id=session_id,
                is_final=True
            )
        
        self.metrics.processing_time = time.time() - start_time
        
        # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
        return {
            "session_id": session_id,
            "metrics": {
                "total_chunks": self.metrics.total_chunks,
                "processed_chunks": self.metrics.processed_chunks,
                "processing_time": round(self.metrics.processing_time, 3),
                "average_chunk_time": round(sum(processing_times) / len(processing_times), 4) if processing_times else 0,
                "throughput": round(self.metrics.processed_chunks / self.metrics.processing_time, 2) if self.metrics.processing_time > 0 else 0,
                "error_count": len(self.metrics.errors)
            },
            "performance": {
                "chunk_sizes": {
                    "min": min(chunk_sizes) if chunk_sizes else 0,
                    "max": max(chunk_sizes) if chunk_sizes else 0,
                    "average": round(sum(chunk_sizes) / len(chunk_sizes), 1) if chunk_sizes else 0
                },
                "processing_times": {
                    "min": round(min(processing_times), 4) if processing_times else 0,
                    "max": round(max(processing_times), 4) if processing_times else 0,
                    "average": round(sum(processing_times) / len(processing_times), 4) if processing_times else 0
                }
            },
            "final_data": self.streaming_parser.get_current_data(session_id),
            "errors": self.metrics.errors
        }


async def ai_completion_example():
    """AIæ™ºèƒ½è¡¥å…¨ç¤ºä¾‹"""
    print("=== AIæ™ºèƒ½è¡¥å…¨ç¤ºä¾‹ ===")
    
    processor = AdvancedJSONProcessor()
    
    # å¤æ‚çš„ä¸å®Œæ•´JSON
    incomplete_json = '''
    {
        "application": {
            "name": "DataAnalyzer",
            "version": "2.0.0",
            "features": [
                {
                    "name": "data_processing",
                    "enabled": true,
                    "config": {
                        "batch_size": 1000,
                        "timeout": 30,
                        "retry_count": 3,
                        "processors": [
                            {"type": "json", "priority": 1},
                            {"type": "csv", "priority": 2
    '''
    
    print(f"åŸå§‹ä¸å®Œæ•´JSON (é•¿åº¦: {len(incomplete_json)} å­—ç¬¦)")
    print(f"é¢„è§ˆ: {incomplete_json[:100]}...")
    
    # æ¨¡æ‹Ÿæ¨¡å‹é…ç½®ï¼ˆå®é™…ä½¿ç”¨æ—¶éœ€è¦çœŸå®çš„APIå¯†é’¥ï¼‰
    model_config = ModelConfig(
        model_type="openai",
        model_name="gpt-3.5-turbo",
        api_key="mock-api-key"  # åœ¨å®é™…ä½¿ç”¨ä¸­æ›¿æ¢ä¸ºçœŸå®å¯†é’¥
    )
    
    try:
        result = await processor.process_with_ai_completion(
            incomplete_json=incomplete_json,
            model_config=model_config
        )
        
        print(f"\nè¡¥å…¨ç»“æœ:")
        print(f"  æ–¹æ³•: {result['method']}")
        print(f"  ç½®ä¿¡åº¦: {result['confidence']:.2f}")
        
        if result['method'] == 'ai':
            print(f"  ä½¿ç”¨æ¨¡å‹: {result.get('model', 'Unknown')}")
        
        # éªŒè¯è¡¥å…¨ç»“æœ
        try:
            completed_data = json.loads(result['result'])
            print(f"  âœ… è¡¥å…¨æˆåŠŸï¼Œæ•°æ®æœ‰æ•ˆ")
            print(f"  åº”ç”¨åç§°: {completed_data.get('application', {}).get('name', 'Unknown')}")
            print(f"  åŠŸèƒ½æ•°é‡: {len(completed_data.get('application', {}).get('features', []))}")
        except json.JSONDecodeError:
            print(f"  âŒ è¡¥å…¨ç»“æœæ— æ•ˆ")
    
    except Exception as e:
        print(f"AIè¡¥å…¨ç¤ºä¾‹å¤±è´¥: {e}")
        print("æç¤º: éœ€è¦æœ‰æ•ˆçš„APIå¯†é’¥æ‰èƒ½ä½¿ç”¨AIè¡¥å…¨åŠŸèƒ½")


async def intelligent_analysis_example():
    """æ™ºèƒ½åˆ†æç¤ºä¾‹"""
    print("\n=== æ™ºèƒ½åˆ†æç¤ºä¾‹ ===")
    
    processor = AdvancedJSONProcessor()
    
    # å¤æ‚çš„æµ‹è¯•æ•°æ®
    complex_data = {
        "system": {
            "name": "DataPlatform",
            "version": "3.1.0",
            "components": [
                {
                    "id": "comp-001",
                    "name": "DataIngestion",
                    "type": "service",
                    "config": {
                        "sources": ["kafka", "rabbitmq", "http"],
                        "batch_size": 1000,
                        "timeout": 30,
                        "retry_policy": {
                            "max_retries": 3,
                            "backoff_factor": 2,
                            "retry_codes": [500, 502, 503]
                        }
                    },
                    "metrics": {
                        "throughput": [100, 150, 200, 180, 220],
                        "latency": [10, 15, 12, 18, 14],
                        "error_rate": [0.01, 0.02, 0.015, 0.025, 0.018]
                    }
                },
                {
                    "id": "comp-002",
                    "name": "DataProcessing",
                    "type": "worker",
                    "config": {
                        "workers": 10,
                        "queue_size": 5000,
                        "processing_timeout": 60
                    },
                    "metrics": {
                        "processed_items": [500, 600, 550, 700, 650],
                        "processing_time": [2.5, 3.1, 2.8, 3.5, 3.0],
                        "memory_usage": [512, 600, 580, 650, 620]
                    }
                }
            ],
            "global_config": {
                "logging": {
                    "level": "INFO",
                    "format": "json",
                    "outputs": ["console", "file", "elasticsearch"]
                },
                "monitoring": {
                    "enabled": True,
                    "interval": 60,
                    "metrics": ["cpu", "memory", "disk", "network"]
                }
            }
        }
    }
    
    print(f"åˆ†æå¤æ‚æ•°æ®ç»“æ„ (å¤§å°: {len(json.dumps(complex_data))} å­—ç¬¦)")
    
    # æ‰§è¡Œä¸åŒç±»å‹çš„åˆ†æ
    analysis_types = ["comprehensive", "performance", "structure"]
    
    for analysis_type in analysis_types:
        print(f"\n--- {analysis_type.upper()} åˆ†æ ---")
        
        result = await processor.intelligent_path_analysis(
            data=complex_data,
            analysis_type=analysis_type
        )
        
        print(f"æ€»è·¯å¾„æ•°: {result['total_paths']}")
        
        if "statistics" in result:
            stats = result["statistics"]
            print(f"ç»Ÿè®¡ä¿¡æ¯:")
            for key, value in stats.items():
                print(f"  {key}: {value}")
        
        if "recommendations" in result:
            print(f"å»ºè®®:")
            for rec in result["recommendations"]:
                print(f"  â€¢ {rec}")
        
        if "patterns" in result and result["patterns"]:
            print(f"æ¨¡å¼:")
            for pattern in result["patterns"][:3]:  # æ˜¾ç¤ºå‰3ä¸ªæ¨¡å¼
                print(f"  â€¢ {pattern}")


async def adaptive_streaming_example():
    """è‡ªé€‚åº”æµå¼å¤„ç†ç¤ºä¾‹"""
    print("\n=== è‡ªé€‚åº”æµå¼å¤„ç†ç¤ºä¾‹ ===")
    
    processor = AdvancedJSONProcessor()
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®æº
    def generate_variable_chunks():
        """ç”Ÿæˆå¯å˜å¤§å°çš„æ•°æ®å—"""
        base_data = {
            "events": [
                {
                    "id": i,
                    "timestamp": f"2024-01-15T{10 + i % 14:02d}:30:00Z",
                    "type": "user_action" if i % 3 == 0 else "system_event",
                    "data": {
                        "user_id": f"user-{i % 100}",
                        "action": f"action-{i % 10}",
                        "metadata": {
                            "source": "web" if i % 2 == 0 else "mobile",
                            "session_id": f"session-{i // 10}",
                            "properties": {
                                "page": f"page-{i % 5}",
                                "duration": i * 10 + 100,
                                "interactions": list(range(i % 5))
                            }
                        }
                    }
                }
                for i in range(50)  # 50ä¸ªäº‹ä»¶
            ]
        }
        
        json_str = json.dumps(base_data, ensure_ascii=False)
        
        # åˆ›å»ºå¯å˜å¤§å°çš„å—
        chunks = []
        i = 0
        while i < len(json_str):
            # éšæœºå—å¤§å°ï¼Œæ¨¡æ‹Ÿç½‘ç»œä¼ è¾“çš„ä¸ç¡®å®šæ€§
            chunk_size = 100 + (i % 200)  # 100-300å­—ç¬¦
            chunk = json_str[i:i + chunk_size]
            chunks.append(chunk)
            i += chunk_size
        
        return chunks
    
    # é…ç½®è‡ªé€‚åº”å¤„ç†
    adaptive_config = {
        "chunk_size_adjustment": True,
        "error_recovery": True,
        "performance_monitoring": True,
        "dynamic_buffering": True
    }
    
    print(f"é…ç½®è‡ªé€‚åº”å¤„ç†:")
    for key, value in adaptive_config.items():
        print(f"  {key}: {'âœ…' if value else 'âŒ'}")
    
    # æ‰§è¡Œè‡ªé€‚åº”å¤„ç†
    session_id = "adaptive-demo"
    
    result = await processor.adaptive_streaming_processing(
        data_source=generate_variable_chunks,
        session_id=session_id,
        adaptive_config=adaptive_config
    )
    
    print(f"\nå¤„ç†ç»“æœ:")
    print(f"ä¼šè¯ID: {result['session_id']}")
    
    metrics = result['metrics']
    print(f"\nå¤„ç†æŒ‡æ ‡:")
    print(f"  æ€»å—æ•°: {metrics['total_chunks']}")
    print(f"  å·²å¤„ç†: {metrics['processed_chunks']}")
    print(f"  å¤„ç†æ—¶é—´: {metrics['processing_time']}ç§’")
    print(f"  ååé‡: {metrics['throughput']} å—/ç§’")
    print(f"  é”™è¯¯æ•°: {metrics['error_count']}")
    
    performance = result['performance']
    print(f"\næ€§èƒ½åˆ†æ:")
    print(f"  å—å¤§å°èŒƒå›´: {performance['chunk_sizes']['min']}-{performance['chunk_sizes']['max']} å­—ç¬¦")
    print(f"  å¹³å‡å—å¤§å°: {performance['chunk_sizes']['average']} å­—ç¬¦")
    print(f"  å¤„ç†æ—¶é—´èŒƒå›´: {performance['processing_times']['min']}-{performance['processing_times']['max']}ç§’")
    print(f"  å¹³å‡å¤„ç†æ—¶é—´: {performance['processing_times']['average']}ç§’")
    
    # éªŒè¯æœ€ç»ˆæ•°æ®
    final_data = result['final_data']
    if final_data:
        print(f"\næ•°æ®éªŒè¯:")
        print(f"  äº‹ä»¶æ•°é‡: {len(final_data.get('events', []))}")
        print(f"  æ•°æ®å®Œæ•´æ€§: âœ…")
    
    if result['errors']:
        print(f"\né”™è¯¯ä¿¡æ¯:")
        for error in result['errors'][:3]:  # æ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
            print(f"  â€¢ {error}")


async def integration_workflow_example():
    """é›†æˆå·¥ä½œæµç¤ºä¾‹"""
    print("\n=== é›†æˆå·¥ä½œæµç¤ºä¾‹ ===")
    
    processor = AdvancedJSONProcessor()
    
    # æ¨¡æ‹Ÿå¤æ‚çš„æ•°æ®å¤„ç†å·¥ä½œæµ
    workflow_data = {
        "pipeline": {
            "id": "data-pipeline-001",
            "name": "CustomerDataProcessing",
            "stages": [
                {
                    "stage_id": "ingestion",
                    "type": "data_source",
                    "config": {
                        "source_type": "api",
                        "endpoint": "https://api.example.com/customers",
                        "batch_size": 1000,
                        "rate_limit": 100
                    },
                    "output_schema": {
                        "customer_id": "string",
                        "name": "string",
                        "email": "string",
                        "created_at": "datetime"
                    }
                },
                {
                    "stage_id": "validation",
                    "type": "data_quality",
                    "config": {
                        "rules": [
                            {"field": "email", "type": "email_format"},
                            {"field": "customer_id", "type": "not_null"},
                            {"field": "name", "type": "min_length", "value": 2}
                        ],
                        "error_threshold": 0.05
                    }
                },
                {
                    "stage_id": "enrichment",
                    "type": "data_enhancement",
                    "config": {
                        "enrichment_sources": [
                            {"type": "geo_location", "field": "ip_address"},
                            {"type": "demographic", "field": "postal_code"}
                        ]
                    }
                }
            ]
        }
    }
    
    print(f"å·¥ä½œæµ: {workflow_data['pipeline']['name']}")
    print(f"é˜¶æ®µæ•°: {len(workflow_data['pipeline']['stages'])}")
    
    # 1. è·¯å¾„åˆ†æ
    print(f"\næ­¥éª¤1: è·¯å¾„åˆ†æ")
    path_analysis = await processor.intelligent_path_analysis(
        data=workflow_data,
        analysis_type="comprehensive"
    )
    
    print(f"  å‘ç° {path_analysis['total_paths']} ä¸ªæ•°æ®è·¯å¾„")
    print(f"  æœ€å¤§åµŒå¥—æ·±åº¦: {path_analysis['statistics']['max_depth']}")
    
    # 2. æ•°æ®åºåˆ—åŒ–å’Œåˆ†å—
    print(f"\næ­¥éª¤2: æ•°æ®åºåˆ—åŒ–")
    json_str = json.dumps(workflow_data, ensure_ascii=False)
    chunks = [json_str[i:i+200] for i in range(0, len(json_str), 200)]
    
    print(f"  åŸå§‹å¤§å°: {len(json_str)} å­—ç¬¦")
    print(f"  åˆ†å—æ•°é‡: {len(chunks)}")
    
    # 3. æµå¼å¤„ç†
    print(f"\næ­¥éª¤3: æµå¼å¤„ç†")
    
    def chunk_generator():
        return chunks
    
    streaming_result = await processor.adaptive_streaming_processing(
        data_source=chunk_generator,
        session_id="workflow-demo",
        adaptive_config={
            "chunk_size_adjustment": True,
            "error_recovery": True,
            "performance_monitoring": True,
            "dynamic_buffering": False  # ç¦ç”¨ç¼“å†²ä»¥ç¡®ä¿å®æ—¶å¤„ç†
        }
    )
    
    print(f"  å¤„ç†æ—¶é—´: {streaming_result['metrics']['processing_time']}ç§’")
    print(f"  ååé‡: {streaming_result['metrics']['throughput']} å—/ç§’")
    
    # 4. ç»“æœéªŒè¯
    print(f"\næ­¥éª¤4: ç»“æœéªŒè¯")
    final_data = streaming_result['final_data']
    
    if final_data:
        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        original_stages = len(workflow_data['pipeline']['stages'])
        processed_stages = len(final_data.get('pipeline', {}).get('stages', []))
        
        print(f"  åŸå§‹é˜¶æ®µæ•°: {original_stages}")
        print(f"  å¤„ç†åé˜¶æ®µæ•°: {processed_stages}")
        print(f"  æ•°æ®å®Œæ•´æ€§: {'âœ… å®Œæ•´' if original_stages == processed_stages else 'âŒ ä¸å®Œæ•´'}")
        
        # éªŒè¯ç‰¹å®šå­—æ®µ
        pipeline_id = final_data.get('pipeline', {}).get('id')
        print(f"  ç®¡é“ID: {pipeline_id}")
        
        # æ£€æŸ¥é…ç½®å®Œæ•´æ€§
        config_paths = []
        for stage in final_data.get('pipeline', {}).get('stages', []):
            if 'config' in stage:
                stage_id = stage.get('stage_id', 'unknown')
                config_paths.append(f"pipeline.stages[{stage_id}].config")
        
        print(f"  é…ç½®è·¯å¾„æ•°: {len(config_paths)}")
        
    else:
        print(f"  âŒ å¤„ç†å¤±è´¥ï¼Œæ— æœ€ç»ˆæ•°æ®")
    
    # 5. æ€§èƒ½æŠ¥å‘Š
    print(f"\næ­¥éª¤5: æ€§èƒ½æŠ¥å‘Š")
    performance = streaming_result['performance']
    
    print(f"  å¹³å‡å—å¤„ç†æ—¶é—´: {performance['processing_times']['average']}ç§’")
    print(f"  æœ€å¿«å¤„ç†æ—¶é—´: {performance['processing_times']['min']}ç§’")
    print(f"  æœ€æ…¢å¤„ç†æ—¶é—´: {performance['processing_times']['max']}ç§’")
    
    efficiency = (streaming_result['metrics']['processed_chunks'] / 
                 streaming_result['metrics']['total_chunks']) * 100
    print(f"  å¤„ç†æ•ˆç‡: {efficiency:.1f}%")
    
    print(f"\nâœ… é›†æˆå·¥ä½œæµå®Œæˆ")


async def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰é«˜çº§ç”¨æ³•ç¤ºä¾‹"""
    print("Agently Format - é«˜çº§ç”¨æ³•ç¤ºä¾‹")
    print("=" * 50)
    
    try:
        await ai_completion_example()
        await intelligent_analysis_example()
        await adaptive_streaming_example()
        await integration_workflow_example()
        
        print("\n=== æ‰€æœ‰é«˜çº§ç”¨æ³•ç¤ºä¾‹è¿è¡Œå®Œæˆ ===")
        print("\né«˜çº§åŠŸèƒ½ç‰¹æ€§:")
        print("ğŸ¤– AIæ™ºèƒ½è¡¥å…¨")
        print("ğŸ” æ™ºèƒ½è·¯å¾„åˆ†æ")
        print("ğŸ”„ è‡ªé€‚åº”æµå¼å¤„ç†")
        print("ğŸ“Š æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–")
        print("ğŸ› ï¸ é”™è¯¯æ¢å¤æœºåˆ¶")
        print("ğŸ”— é›†æˆå·¥ä½œæµ")
        
        print("\nåº”ç”¨åœºæ™¯:")
        print("â€¢ å¤§è§„æ¨¡æ•°æ®å¤„ç†")
        print("â€¢ å®æ—¶æ•°æ®æµåˆ†æ")
        print("â€¢ å¤æ‚JSONç»“æ„å¤„ç†")
        print("â€¢ æ™ºèƒ½æ•°æ®è¡¥å…¨")
        print("â€¢ æ€§èƒ½ä¼˜åŒ–å’Œç›‘æ§")
        
    except Exception as e:
        print(f"\nè¿è¡Œé«˜çº§ç”¨æ³•ç¤ºä¾‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹
    asyncio.run(main())