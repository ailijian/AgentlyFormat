# AgentlyFormat åŠŸèƒ½ä»‹ç»ä¸ä½¿ç”¨æŒ‡å¯¼æ‰‹å†Œ

## ğŸ“‹ ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
2. [æ ¸å¿ƒåŠŸèƒ½](#æ ¸å¿ƒåŠŸèƒ½)
3. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
4. [è¯¦ç»†åŠŸèƒ½ä»‹ç»](#è¯¦ç»†åŠŸèƒ½ä»‹ç»)
5. [APIæ–‡æ¡£](#apiæ–‡æ¡£)
6. [é…ç½®æŒ‡å—](#é…ç½®æŒ‡å—)
7. [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
8. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
9. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
10. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## ğŸ“– é¡¹ç›®æ¦‚è¿°

### ä»€ä¹ˆæ˜¯ AgentlyFormatï¼Ÿ

AgentlyFormat æ˜¯ä¸€ä¸ªä¸“æ³¨äºå¤§æ¨¡å‹æ ¼å¼åŒ–è¾“å‡ºç»“æœçš„Pythonåº“ï¼Œä¸»è¦è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨ç”ŸæˆJSONæ ¼å¼æ•°æ®æ—¶é‡åˆ°çš„å„ç§é—®é¢˜ã€‚è¯¥é¡¹ç›®åŸºäºAgentlyå¼ºå¤§çš„æ ¼å¼åŒ–è¾“å‡ºèƒ½åŠ›æ„å»ºï¼Œä¸»æ‰“è½»é‡åŒ–å’Œé«˜æ€§èƒ½ã€‚

### æ ¸å¿ƒé—®é¢˜è§£å†³

- **JSONè¾“å‡ºä¸å®Œæ•´**: å¤§æ¨¡å‹ç»å¸¸ç”Ÿæˆä¸å®Œæ•´çš„JSONæ•°æ®
- **æµå¼è¾“å‡ºå¤„ç†**: å®æ—¶å¤„ç†æµå¼JSONæ•°æ®æµ
- **æ ¼å¼ä¸ä¸€è‡´**: ä¸åŒæ¨¡å‹è¾“å‡ºæ ¼å¼å·®å¼‚è¾ƒå¤§
- **ç»“æ„å¤æ‚**: å¤æ‚åµŒå¥—JSONç»“æ„éš¾ä»¥å¤„ç†
- **æ€§èƒ½ç“¶é¢ˆ**: å¤§æ–‡ä»¶å¤„ç†æ€§èƒ½é—®é¢˜

### ç‰ˆæœ¬ä¿¡æ¯

- **å½“å‰ç‰ˆæœ¬**: v2.0.0
- **Pythonè¦æ±‚**: â‰¥ 3.8
- **è®¸å¯è¯**: Apache-2.0
- **ç»´æŠ¤çŠ¶æ€**: ç§¯æç»´æŠ¤

---

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½

### 1. æ™ºèƒ½JSONè¡¥å…¨
- è‡ªåŠ¨è¡¥å…¨ä¸å®Œæ•´çš„JSONå­—ç¬¦ä¸²
- æ”¯æŒå¤šç§è¡¥å…¨ç­–ç•¥ï¼ˆæ™ºèƒ½ã€ä¿å®ˆã€æ¿€è¿›ï¼‰
- åŒé˜¶æ®µè¡¥å…¨ï¼šè¯æ³•åˆ†æ â†’ è¯­æ³•ä¿®å¤
- æ™ºèƒ½ç±»å‹æ¨æ–­å’Œé”™è¯¯ä¿®å¤

### 2. æµå¼JSONè§£æ
- å®æ—¶å¤„ç†æµå¼JSONæ•°æ®
- è·¨å—ç¼“å†²æœºåˆ¶ï¼Œæ”¯æŒå¤§æ–‡ä»¶å¤„ç†
- æ™ºèƒ½è¾¹ç•Œæ£€æµ‹ï¼Œé¿å…æ•°æ®æˆªæ–­
- äº‹ä»¶é©±åŠ¨çš„è§£æè¿›åº¦é€šçŸ¥

### 3. æ•°æ®è·¯å¾„æ„å»º
- è‡ªåŠ¨æå–JSONæ•°æ®çš„æ‰€æœ‰è·¯å¾„
- æ”¯æŒå¤šç§è·¯å¾„æ ¼å¼ï¼ˆç‚¹å·ã€æ–œæ ã€æ‹¬å·ï¼‰
- è·¯å¾„æ ¼å¼è½¬æ¢å’ŒéªŒè¯
- é€šè¿‡è·¯å¾„å¿«é€Ÿè®¿é—®æ•°æ®

### 4. SchemaéªŒè¯
- å¢é‡SchemaéªŒè¯æœºåˆ¶
- è‡ªå®šä¹‰éªŒè¯è§„åˆ™æ”¯æŒ
- ç±»å‹å…¼å®¹æ€§æ£€æŸ¥
- ä¿®å¤å»ºè®®å’Œé”™è¯¯å®šä½

### 5. å·®åˆ†å¼•æ“
- ç»“æ„åŒ–æ•°æ®å·®åˆ†ç®—æ³•
- å¢é‡æ›´æ–°å’Œç‰ˆæœ¬è¿½è¸ª
- äº‹ä»¶å»é‡å’Œåˆå¹¶ä¼˜åŒ–
- å†²çªæ£€æµ‹å’Œè§£å†³

### 6. å¤šæ¨¡å‹é€‚é…å™¨
- æ”¯æŒä¸»æµå¤§æ¨¡å‹APIï¼ˆOpenAIã€è±†åŒ…ã€æ–‡å¿ƒã€åƒé—®ã€DeepSeekã€Kimiï¼‰
- ç»Ÿä¸€çš„æ¥å£æŠ½è±¡
- è‡ªåŠ¨é‡è¯•å’Œé”™è¯¯å¤„ç†
- æµå¼å’Œéæµå¼å“åº”æ”¯æŒ

---

## âš¡ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# åŸºç¡€å®‰è£…
pip install AgentlyFormat

# å¼€å‘ç¯å¢ƒå®‰è£…
pip install -e ".[dev]"

# å®Œæ•´åŠŸèƒ½å®‰è£…
pip install -e ".[dev,docs]"
```

### åŸºç¡€ä½¿ç”¨

```python
from agently_format.core.json_completer import JSONCompleter
from agently_format.core.streaming_parser import StreamingParser
from agently_format.core.path_builder import PathBuilder

# JSONè¡¥å…¨
completer = JSONCompleter()
result = completer.complete('{"name": "Alice", "age": 25')
print(result.completed_json)

# æµå¼è§£æ
parser = StreamingParser()
session_id = parser.create_session()
result = await parser.parse_chunk(session_id, '{"users": [')

# è·¯å¾„æ„å»º
builder = PathBuilder()
data = {"user": {"name": "Alice", "profile": {"age": 25}}}
paths = builder.extract_parsing_key_orders(data)
print(paths)  # ['user.name', 'user.profile.age']
```

### å¯åŠ¨APIæœåŠ¡

```bash
# å¯åŠ¨å¼€å‘æœåŠ¡å™¨
python -m agently_format.api.app

# æˆ–ä½¿ç”¨uvicorn
uvicorn agently_format.api.app:app --host 0.0.0.0 --port 8000 --reload
```

---

## ğŸ“š è¯¦ç»†åŠŸèƒ½ä»‹ç»

### JSONæ™ºèƒ½è¡¥å…¨å™¨ (JSONCompleter)

#### åŠŸèƒ½ç‰¹æ€§
- **å¤šç­–ç•¥è¡¥å…¨**: æ”¯æŒæ™ºèƒ½ã€ä¿å®ˆã€æ¿€è¿›ä¸‰ç§è¡¥å…¨ç­–ç•¥
- **åŒé˜¶æ®µå¤„ç†**: è¯æ³•åˆ†æå’Œè¯­æ³•ä¿®å¤çš„ä¸¤é˜¶æ®µå¤„ç†
- **RepairTraceæœºåˆ¶**: å®Œæ•´çš„ä¿®å¤è·¯å¾„è¿½è¸ªå’Œå›æ»š
- **ç±»å‹æ¨æ–­**: åŸºäºä¸Šä¸‹æ–‡çš„æ™ºèƒ½ç±»å‹æ¨æ–­
- **å¢é‡ä¿®å¤**: æ”¯æŒéƒ¨åˆ†å†…å®¹çš„å¢é‡è¡¥å…¨

#### ä½¿ç”¨æ–¹æ³•

```python
from agently_format.core.json_completer import JSONCompleter, CompletionStrategy

completer = JSONCompleter()

# åŸºç¡€è¡¥å…¨
incomplete_json = '{"name": "Alice", "age": 25'
result = completer.complete(incomplete_json)

# æŒ‡å®šç­–ç•¥è¡¥å…¨
result = completer.complete(
    incomplete_json, 
    strategy=CompletionStrategy.CONSERVATIVE
)

# æ£€æŸ¥ç»“æœ
if result.is_valid:
    print(f"è¡¥å…¨æˆåŠŸ: {result.completed_json}")
    print(f"ç½®ä¿¡åº¦: {result.confidence}")
else:
    print(f"è¡¥å…¨å¤±è´¥: {result.error_message}")
```

#### è¡¥å…¨ç­–ç•¥è¯´æ˜

| ç­–ç•¥ | æè¿° | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| SMART | æ™ºèƒ½è¡¥å…¨ï¼Œå¹³è¡¡å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ | å¤§å¤šæ•°åœºæ™¯ |
| CONSERVATIVE | ä¿å®ˆè¡¥å…¨ï¼Œä¼˜å…ˆä¿è¯å‡†ç¡®æ€§ | å…³é”®æ•°æ®å¤„ç† |
| AGGRESSIVE | æ¿€è¿›è¡¥å…¨ï¼Œå°½å¯èƒ½è¡¥å…¨æ›´å¤šå†…å®¹ | æ•°æ®æ¢ç´¢é˜¶æ®µ |

### æµå¼JSONè§£æå™¨ (StreamingParser)

#### åŠŸèƒ½ç‰¹æ€§
- **è·¨å—ç¼“å†²**: ç¯å½¢ç¼“å†²åŒºæ”¯æŒå¤§æ–‡ä»¶æµå¼å¤„ç†
- **æ™ºèƒ½è¾¹ç•Œæ£€æµ‹**: æ‹¬å·/å¼•å·å¹³è¡¡ç»Ÿè®¡ï¼Œé¿å…æ•°æ®æˆªæ–­
- **è½¯è£å‰ªé€»è¾‘**: æ™ºèƒ½è¯†åˆ«å®‰å…¨åˆ‡åˆ†ç‚¹
- **äº‹ä»¶é©±åŠ¨**: å®æ—¶è§£æè¿›åº¦å’ŒçŠ¶æ€é€šçŸ¥
- **ä¼šè¯ç®¡ç†**: æ”¯æŒå¤šä¼šè¯å¹¶å‘å¤„ç†

#### ä½¿ç”¨æ–¹æ³•

```python
import asyncio
from agently_format.core.streaming_parser import StreamingParser

async def streaming_parse_example():
    parser = StreamingParser()
    session_id = parser.create_session()
    
    # æ¨¡æ‹Ÿåˆ†å—æ•°æ®
    chunks = [
        '{"users": [',
        '{"id": 1, "name": "Alice"},',
        '{"id": 2, "name": "Bob"}',
        '], "total": 2}'
    ]
    
    # é€å—è§£æ
    for i, chunk in enumerate(chunks):
        is_final = (i == len(chunks) - 1)
        result = await parser.parse_chunk(
            session_id=session_id,
            chunk=chunk
        )
        
        # è·å–å½“å‰è§£æçŠ¶æ€
        state = parser.parsing_states[session_id]
        print(f"å¤„ç†è¿›åº¦: {state.processed_chunks}/{state.total_chunks}")
    
    # è·å–æœ€ç»ˆç»“æœ
    final_data = parser.get_current_data(session_id)
    print(f"è§£æç»“æœ: {final_data}")
    
    # æ¸…ç†ä¼šè¯
    parser.cleanup_session(session_id)

# è¿è¡Œç¤ºä¾‹
asyncio.run(streaming_parse_example())
```

#### äº‹ä»¶ç›‘å¬

```python
from agently_format.core.event_system import get_global_emitter, EventType

# è·å–å…¨å±€äº‹ä»¶å‘å°„å™¨
emitter = get_global_emitter()

# äº‹ä»¶å¤„ç†å™¨
async def on_parse_progress(event):
    print(f"è§£æè¿›åº¦: {event.data}")

async def on_parse_error(event):
    print(f"è§£æé”™è¯¯: {event.data}")

# æ³¨å†Œäº‹ä»¶ç›‘å¬å™¨
emitter.on(EventType.DELTA, on_parse_progress)
emitter.on(EventType.ERROR, on_parse_error)
```

### æ•°æ®è·¯å¾„æ„å»ºå™¨ (PathBuilder)

#### åŠŸèƒ½ç‰¹æ€§
- **å¤šæ ¼å¼æ”¯æŒ**: ç‚¹å·ã€æ–œæ ã€æ‹¬å·ç­‰å¤šç§è·¯å¾„æ ¼å¼
- **è·¯å¾„æå–**: è‡ªåŠ¨æå–JSONæ•°æ®çš„æ‰€æœ‰è·¯å¾„
- **æ ¼å¼è½¬æ¢**: ä¸åŒè·¯å¾„æ ¼å¼ä¹‹é—´çš„è½¬æ¢
- **å€¼è®¿é—®**: é€šè¿‡è·¯å¾„å¿«é€Ÿè®¿é—®æ•°æ®å€¼
- **è·¯å¾„éªŒè¯**: è·¯å¾„æœ‰æ•ˆæ€§æ£€æŸ¥

#### ä½¿ç”¨æ–¹æ³•

```python
from agently_format.core.path_builder import PathBuilder, PathStyle

builder = PathBuilder()

# ç¤ºä¾‹æ•°æ®
data = {
    "api": {
        "version": "v1",
        "endpoints": [
            {
                "path": "/users",
                "methods": ["GET", "POST"]
            }
        ]
    }
}

# æå–æ‰€æœ‰è·¯å¾„
paths = builder.extract_parsing_key_orders(data)
print("æ‰€æœ‰è·¯å¾„:")
for path in paths:
    print(f"  {path}")

# é€šè¿‡è·¯å¾„è·å–å€¼
success, value = builder.get_value_at_path(data, "api.version")
if success:
    print(f"api.version = {value}")

# è·¯å¾„æ ¼å¼è½¬æ¢
dot_path = "api.endpoints[0].methods"
slash_path = builder.convert_path(dot_path, PathStyle.SLASH)
bracket_path = builder.convert_path(dot_path, PathStyle.BRACKET)

print(f"ç‚¹å·æ ¼å¼: {dot_path}")
print(f"æ–œæ æ ¼å¼: {slash_path}")
print(f"æ‹¬å·æ ¼å¼: {bracket_path}")
```

#### è·¯å¾„æ ¼å¼è¯´æ˜

| æ ¼å¼ | ç¤ºä¾‹ | æè¿° |
|------|------|------|
| DOT | `user.profile.age` | ç‚¹å·åˆ†éš”ï¼Œæœ€å¸¸ç”¨ |
| SLASH | `user/profile/age` | æ–œæ åˆ†éš”ï¼Œç±»ä¼¼æ–‡ä»¶è·¯å¾„ |
| BRACKET | `user[profile][age]` | æ‹¬å·æ ¼å¼ï¼Œæ”¯æŒç‰¹æ®Šå­—ç¬¦ |

### SchemaéªŒè¯å™¨ (SchemaValidator)

#### åŠŸèƒ½ç‰¹æ€§
- **å¢é‡éªŒè¯**: é€è·¯å¾„éªŒè¯æœºåˆ¶ï¼Œæ”¯æŒå®æ—¶éªŒè¯
- **è‡ªå®šä¹‰è§„åˆ™**: æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰éªŒè¯é€»è¾‘
- **ç±»å‹æ£€æŸ¥**: ä¸¥æ ¼çš„ç±»å‹åŒ¹é…å’Œè½¬æ¢
- **é”™è¯¯å®šä½**: ç²¾ç¡®çš„é”™è¯¯ä½ç½®å’Œä¿®å¤å»ºè®®
- **ç¼“å­˜ä¼˜åŒ–**: æ™ºèƒ½ç¼“å­˜å¸¸è§ä¿®å¤æ–¹æ¡ˆ

#### ä½¿ç”¨æ–¹æ³•

```python
from agently_format.core.schemas import SchemaValidator
from pydantic import BaseModel
from typing import List, Optional

# å®šä¹‰Schema
class UserProfile(BaseModel):
    name: str
    age: int
    email: Optional[str] = None
    tags: List[str] = []

class UserData(BaseModel):
    users: List[UserProfile]
    total: int

# åˆ›å»ºéªŒè¯å™¨
validator = SchemaValidator(UserData)

# éªŒè¯æ•°æ®
data = {
    "users": [
        {"name": "Alice", "age": 25, "email": "alice@example.com"},
        {"name": "Bob", "age": "30"}  # ageç±»å‹é”™è¯¯
    ],
    "total": 2
}

result = validator.validate(data)
if result.is_valid:
    print("éªŒè¯é€šè¿‡")
else:
    print("éªŒè¯å¤±è´¥:")
    for error in result.errors:
        print(f"  {error.path}: {error.message}")
        if error.suggestion:
            print(f"    å»ºè®®: {error.suggestion}")
```

### å·®åˆ†å¼•æ“ (DiffEngine)

#### åŠŸèƒ½ç‰¹æ€§
- **ç»“æ„åŒ–å·®åˆ†**: dict/list awareçš„æ™ºèƒ½å·®åˆ†ç®—æ³•
- **å¢é‡æ›´æ–°**: æœ€å°åŒ–æ•°æ®ä¼ è¾“çš„å¢é‡æ›´æ–°
- **ç‰ˆæœ¬è¿½è¸ª**: å®Œæ•´çš„æ•°æ®å˜æ›´å†å²è®°å½•
- **äº‹ä»¶ä¼˜åŒ–**: äº‹ä»¶å»é‡å’Œåˆå¹¶ä¼˜åŒ–
- **å†²çªè§£å†³**: æ™ºèƒ½çš„æ•°æ®å†²çªæ£€æµ‹å’Œè§£å†³

#### ä½¿ç”¨æ–¹æ³•

```python
from agently_format.core.diff_engine import DiffEngine

engine = DiffEngine()

# åŸå§‹æ•°æ®
old_data = {
    "users": [
        {"id": 1, "name": "Alice", "age": 25},
        {"id": 2, "name": "Bob", "age": 30}
    ],
    "total": 2
}

# æ›´æ–°åæ•°æ®
new_data = {
    "users": [
        {"id": 1, "name": "Alice", "age": 26},  # å¹´é¾„å˜æ›´
        {"id": 2, "name": "Bob", "age": 30},
        {"id": 3, "name": "Charlie", "age": 28}  # æ–°å¢ç”¨æˆ·
    ],
    "total": 3
}

# è®¡ç®—å·®åˆ†
diff_result = engine.compute_diff(old_data, new_data)

print("æ•°æ®å˜æ›´:")
for change in diff_result.changes:
    print(f"  {change.operation}: {change.path} = {change.new_value}")

# åº”ç”¨å·®åˆ†
patched_data = engine.apply_diff(old_data, diff_result)
print(f"åº”ç”¨å·®åˆ†å: {patched_data == new_data}")  # True
```

---

## ğŸŒ APIæ–‡æ¡£

### REST APIæ¥å£

#### åŸºç¡€ä¿¡æ¯
- **åŸºç¡€URL**: `http://localhost:8000`
- **APIç‰ˆæœ¬**: v1
- **å†…å®¹ç±»å‹**: `application/json`
- **è®¤è¯æ–¹å¼**: Bearer Tokenï¼ˆå¯é€‰ï¼‰

#### æ ¸å¿ƒæ¥å£

##### 1. JSONè¡¥å…¨æ¥å£

```http
POST /api/v1/json/complete
Content-Type: application/json

{
  "content": "{\"name\": \"Alice\", \"age\": 25",
  "strategy": "smart"
}
```

**å“åº”ç¤ºä¾‹**:
```json
{
  "success": true,
  "data": {
    "completed_json": "{\"name\": \"Alice\", \"age\": 25}",
    "is_valid": true,
    "confidence": 0.95,
    "completion_applied": true
  }
}
```

##### 2. æµå¼è§£ææ¥å£

```http
POST /api/v1/parse/stream
Content-Type: application/json

{
  "session_id": "session_123",
  "chunk": "{\"users\": [",
  "is_final": false
}
```

**å“åº”ç¤ºä¾‹**:
```json
{
  "success": true,
  "data": {
    "session_id": "session_123",
    "parsed_data": {"users": []},
    "is_complete": false,
    "progress": 0.25
  }
}
```

##### 3. è·¯å¾„æ„å»ºæ¥å£

```http
POST /api/v1/path/build
Content-Type: application/json

{
  "data": {"user": {"name": "Alice"}},
  "style": "dot"
}
```

**å“åº”ç¤ºä¾‹**:
```json
{
  "success": true,
  "data": {
    "paths": ["user.name"],
    "total_count": 1,
    "style": "dot"
  }
}
```

##### 4. æ¨¡å‹èŠå¤©æ¥å£

```http
POST /api/v1/chat
Content-Type: application/json

{
  "model": "openai",
  "messages": [
    {"role": "user", "content": "è¯·ç”Ÿæˆä¸€ä¸ªç”¨æˆ·ä¿¡æ¯çš„JSON"}
  ],
  "stream": false
}
```

##### 5. ç³»ç»Ÿç»Ÿè®¡æ¥å£

```http
GET /api/v1/stats
```

**å“åº”ç¤ºä¾‹**:
```json
{
  "success": true,
  "data": {
    "total_sessions": 156,
    "active_sessions": 12,
    "total_events": 2847,
    "uptime": 86400,
    "memory_usage": "45.2MB"
  }
}
```

##### 6. å¥åº·æ£€æŸ¥æ¥å£

```http
GET /health
```

**å“åº”ç¤ºä¾‹**:
```json
{
  "status": "healthy",
  "version": "2.0.0",
  "timestamp": "2024-12-01T10:30:00Z",
  "checks": {
    "database": "ok",
    "memory": "ok",
    "disk": "ok"
  }
}
```

### WebSocketæ¥å£

#### è¿æ¥åœ°å€
```
ws://localhost:8000/ws
```

#### æ¶ˆæ¯æ ¼å¼

**å‘é€æ¶ˆæ¯**:
```json
{
  "type": "parse",
  "session_id": "session_123",
  "data": "{\"partial\": \"json\"}"
}
```

**æ¥æ”¶æ¶ˆæ¯**:
```json
{
  "type": "parse_result",
  "session_id": "session_123",
  "data": {
    "parsed_data": {"partial": "json"},
    "is_complete": false,
    "progress": 0.5
  }
}
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
import asyncio
import websockets
import json

async def websocket_client():
    uri = "ws://localhost:8000/ws"
    async with websockets.connect(uri) as websocket:
        # å‘é€è§£æè¯·æ±‚
        message = {
            "type": "parse",
            "session_id": "demo_session",
            "data": '{"users": ['
        }
        await websocket.send(json.dumps(message))
        
        # æ¥æ”¶å“åº”
        response = await websocket.recv()
        result = json.loads(response)
        print(f"å®æ—¶å“åº”: {result}")

asyncio.run(websocket_client())
```

---

## âš™ï¸ é…ç½®æŒ‡å—

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# APIæœåŠ¡é…ç½®
AGENTLY_FORMAT_HOST=0.0.0.0
AGENTLY_FORMAT_PORT=8000
AGENTLY_FORMAT_DEBUG=false

# æ¨¡å‹APIå¯†é’¥
OPENAI_API_KEY=your-openai-key
DOUBAO_API_KEY=your-doubao-key
WENXIN_API_KEY=your-wenxin-key
WENXIN_SECRET_KEY=your-wenxin-secret
QIANWEN_API_KEY=your-qianwen-key
DEEPSEEK_API_KEY=your-deepseek-key
KIMI_API_KEY=your-kimi-key

# æ€§èƒ½é…ç½®
MAX_CHUNK_SIZE=1048576  # 1MB
SESSION_TTL=3600       # 1å°æ—¶
MAX_SESSIONS=1000

# å®‰å…¨é…ç½®
RATE_LIMIT_ENABLED=true
ACCESS_TOKEN=your-access-token
CORS_ORIGINS=*
```

### é…ç½®æ–‡ä»¶ (config.yaml)

```yaml
# æœåŠ¡å™¨é…ç½®
server:
  host: "0.0.0.0"
  port: 8000
  debug: false
  workers: 4

# å¤„ç†é…ç½®
processing:
  max_chunk_size: 1048576  # 1MB
  session_ttl: 3600       # 1å°æ—¶
  max_sessions: 1000
  buffer_size: 8192
  enable_compression: true

# æ¨¡å‹é…ç½®
models:
  openai:
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    timeout: 30
    max_retries: 3
  
  doubao:
    api_key: "${DOUBAO_API_KEY}"
    base_url: "https://ark.cn-beijing.volces.com/api/v3"
    timeout: 30
    max_retries: 3
  
  wenxin:
    api_key: "${WENXIN_API_KEY}"
    api_secret: "${WENXIN_SECRET_KEY}"
    timeout: 30
    max_retries: 3

# å®‰å…¨é…ç½®
security:
  rate_limit_enabled: true
  rate_limit_requests: 100
  rate_limit_window: 60  # ç§’
  access_token: "${ACCESS_TOKEN}"
  cors_origins:
    - "*"
  cors_methods:
    - "GET"
    - "POST"
    - "PUT"
    - "DELETE"

# ç›‘æ§é…ç½®
monitoring:
  metrics_enabled: true
  health_check_enabled: true
  log_level: "INFO"
  log_format: "json"
  
# ç¼“å­˜é…ç½®
cache:
  enabled: true
  ttl: 300  # 5åˆ†é’Ÿ
  max_size: 1000
```

### ç¨‹åºåŒ–é…ç½®

```python
from agently_format.api.config import Settings
from agently_format.core.streaming_parser import StreamingParser
from agently_format.core.json_completer import JSONCompleter

# åˆ›å»ºè‡ªå®šä¹‰é…ç½®
settings = Settings(
    host="0.0.0.0",
    port=8000,
    debug=False,
    max_chunk_size=1024*1024,  # 1MB
    session_ttl=3600,  # 1å°æ—¶
    rate_limit_enabled=True
)

# ä½¿ç”¨é…ç½®åˆ›å»ºç»„ä»¶
parser = StreamingParser(
    max_chunk_size=settings.max_chunk_size,
    session_ttl=settings.session_ttl
)

completer = JSONCompleter(
    default_strategy="smart",
    enable_cache=True
)
```

---

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1: å¤„ç†å¤§æ¨¡å‹æµå¼è¾“å‡º

```python
import asyncio
from agently_format.adapters.openai_adapter import OpenAIAdapter
from agently_format.core.streaming_parser import StreamingParser
from agently_format.core.json_completer import JSONCompleter

async def process_llm_stream():
    # åˆ›å»ºæ¨¡å‹é€‚é…å™¨
    adapter = OpenAIAdapter(api_key="your-api-key")
    
    # åˆ›å»ºæµå¼è§£æå™¨
    parser = StreamingParser()
    session_id = parser.create_session()
    
    # åˆ›å»ºJSONè¡¥å…¨å™¨
    completer = JSONCompleter()
    
    # æ„å»ºèŠå¤©æ¶ˆæ¯
    messages = [
        {
            "role": "user", 
            "content": "è¯·ç”Ÿæˆ5ä¸ªç”¨æˆ·çš„ä¿¡æ¯ï¼ŒåŒ…å«å§“åã€å¹´é¾„ã€é‚®ç®±ï¼Œä»¥JSONæ ¼å¼è¿”å›"
        }
    ]
    
    print("å¼€å§‹å¤„ç†å¤§æ¨¡å‹æµå¼è¾“å‡º...")
    
    # æµå¼è°ƒç”¨æ¨¡å‹
    async for chunk in adapter.chat_completion_stream(
        messages=messages,
        temperature=0.7
    ):
        if chunk.content:
            # å®æ—¶è§£æJSONå—
            result = await parser.parse_chunk(
                session_id=session_id,
                chunk=chunk.content
            )
            
            # è·å–å½“å‰è§£æçŠ¶æ€
            state = parser.parsing_states[session_id]
            if state.current_data:
                print(f"å½“å‰è§£ææ•°æ®: {state.current_data}")
    
    # è·å–æœ€ç»ˆæ•°æ®
    final_data = parser.get_current_data(session_id)
    
    # å¦‚æœæ•°æ®ä¸å®Œæ•´ï¼Œè¿›è¡Œè¡¥å…¨
    if not final_data:
        raw_content = parser.get_raw_content(session_id)
        completion_result = completer.complete(raw_content)
        
        if completion_result.is_valid:
            print(f"è¡¥å…¨åçš„JSON: {completion_result.completed_json}")
        else:
            print(f"è¡¥å…¨å¤±è´¥: {completion_result.error_message}")
    else:
        print(f"è§£ææˆåŠŸ: {final_data}")
    
    # æ¸…ç†èµ„æº
    parser.cleanup_session(session_id)
    await adapter.close()

# è¿è¡Œç¤ºä¾‹
asyncio.run(process_llm_stream())
```

### ç¤ºä¾‹2: æ‰¹é‡æ•°æ®å¤„ç†

```python
import asyncio
import json
from typing import List, Dict, Any
from agently_format.core.json_completer import JSONCompleter
from agently_format.core.path_builder import PathBuilder
from agently_format.core.schemas import SchemaValidator
from pydantic import BaseModel

class UserInfo(BaseModel):
    name: str
    age: int
    email: str
    department: str

class BatchProcessor:
    def __init__(self):
        self.completer = JSONCompleter()
        self.path_builder = PathBuilder()
        self.validator = SchemaValidator(UserInfo)
    
    async def process_batch(self, incomplete_jsons: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡å¤„ç†ä¸å®Œæ•´çš„JSONæ•°æ®"""
        results = []
        
        for i, incomplete_json in enumerate(incomplete_jsons):
            print(f"å¤„ç†ç¬¬ {i+1}/{len(incomplete_jsons)} æ¡æ•°æ®...")
            
            # 1. JSONè¡¥å…¨
            completion_result = self.completer.complete(incomplete_json)
            
            if not completion_result.is_valid:
                results.append({
                    "index": i,
                    "status": "completion_failed",
                    "error": completion_result.error_message,
                    "original": incomplete_json
                })
                continue
            
            # 2. è§£æJSON
            try:
                data = json.loads(completion_result.completed_json)
            except json.JSONDecodeError as e:
                results.append({
                    "index": i,
                    "status": "parse_failed",
                    "error": str(e),
                    "completed_json": completion_result.completed_json
                })
                continue
            
            # 3. æå–æ•°æ®è·¯å¾„
            paths = self.path_builder.extract_parsing_key_orders(data)
            
            # 4. SchemaéªŒè¯
            validation_result = self.validator.validate(data)
            
            # 5. æ„å»ºç»“æœ
            result = {
                "index": i,
                "status": "success" if validation_result.is_valid else "validation_failed",
                "original": incomplete_json,
                "completed": completion_result.completed_json,
                "parsed_data": data,
                "paths": paths,
                "confidence": completion_result.confidence
            }
            
            if not validation_result.is_valid:
                result["validation_errors"] = [
                    {"path": err.path, "message": err.message}
                    for err in validation_result.errors
                ]
            
            results.append(result)
        
        return results
    
    def generate_report(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        total = len(results)
        success = sum(1 for r in results if r["status"] == "success")
        completion_failed = sum(1 for r in results if r["status"] == "completion_failed")
        parse_failed = sum(1 for r in results if r["status"] == "parse_failed")
        validation_failed = sum(1 for r in results if r["status"] == "validation_failed")
        
        avg_confidence = sum(
            r.get("confidence", 0) for r in results 
            if r["status"] in ["success", "validation_failed"]
        ) / max(1, success + validation_failed)
        
        return {
            "total_processed": total,
            "success_count": success,
            "completion_failed_count": completion_failed,
            "parse_failed_count": parse_failed,
            "validation_failed_count": validation_failed,
            "success_rate": success / total if total > 0 else 0,
            "average_confidence": avg_confidence,
            "failed_items": [
                {"index": r["index"], "status": r["status"], "error": r.get("error")}
                for r in results if r["status"] != "success"
            ]
        }

# ä½¿ç”¨ç¤ºä¾‹
async def batch_processing_example():
    processor = BatchProcessor()
    
    # æ¨¡æ‹Ÿä¸å®Œæ•´çš„JSONæ•°æ®
    incomplete_jsons = [
        '{"name": "Alice", "age": 25, "email": "alice@example.com"',
        '{"name": "Bob", "age": 30, "email": "bob@example.com", "department": "IT"',
        '{"name": "Charlie", "age": "invalid", "email": "charlie@example.com"',
        '{"name": "David", "age": 28',
        '{"name": "Eve", "age": 32, "email": "eve@example.com", "department": "HR"}'
    ]
    
    # æ‰¹é‡å¤„ç†
    results = await processor.process_batch(incomplete_jsons)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = processor.generate_report(results)
    
    print("\n=== å¤„ç†æŠ¥å‘Š ===")
    print(f"æ€»å¤„ç†æ•°é‡: {report['total_processed']}")
    print(f"æˆåŠŸæ•°é‡: {report['success_count']}")
    print(f"æˆåŠŸç‡: {report['success_rate']:.2%}")
    print(f"å¹³å‡ç½®ä¿¡åº¦: {report['average_confidence']:.2f}")
    
    if report['failed_items']:
        print("\nå¤±è´¥é¡¹ç›®:")
        for item in report['failed_items']:
            print(f"  ç´¢å¼• {item['index']}: {item['status']} - {item['error']}")
    
    print("\n=== è¯¦ç»†ç»“æœ ===")
    for result in results:
        print(f"ç´¢å¼• {result['index']}: {result['status']}")
        if result['status'] == 'success':
            print(f"  æ•°æ®: {result['parsed_data']}")
            print(f"  è·¯å¾„æ•°é‡: {len(result['paths'])}")

# è¿è¡Œç¤ºä¾‹
asyncio.run(batch_processing_example())
```

### ç¤ºä¾‹3: å®æ—¶æ•°æ®ç›‘æ§

```python
import asyncio
import json
import time
from typing import Dict, Any, List
from dataclasses import dataclass, field
from agently_format.core.streaming_parser import StreamingParser
from agently_format.core.event_system import get_global_emitter, EventType
from agently_format.core.diff_engine import DiffEngine

@dataclass
class MonitoringMetrics:
    """ç›‘æ§æŒ‡æ ‡"""
    total_events: int = 0
    parse_events: int = 0
    error_events: int = 0
    active_sessions: int = 0
    avg_parse_time: float = 0.0
    parse_times: List[float] = field(default_factory=list)
    last_update: float = field(default_factory=time.time)

class RealTimeMonitor:
    """å®æ—¶æ•°æ®ç›‘æ§å™¨"""
    
    def __init__(self):
        self.parser = StreamingParser()
        self.diff_engine = DiffEngine()
        self.emitter = get_global_emitter()
        self.metrics = MonitoringMetrics()
        self.data_snapshots: Dict[str, Any] = {}
        
        # æ³¨å†Œäº‹ä»¶ç›‘å¬å™¨
        self._setup_event_listeners()
    
    def _setup_event_listeners(self):
        """è®¾ç½®äº‹ä»¶ç›‘å¬å™¨"""
        self.emitter.on(EventType.DELTA, self._on_parse_event)
        self.emitter.on(EventType.ERROR, self._on_error_event)
        self.emitter.on(EventType.COMPLETE, self._on_complete_event)
    
    async def _on_parse_event(self, event):
        """è§£æäº‹ä»¶å¤„ç†"""
        self.metrics.total_events += 1
        self.metrics.parse_events += 1
        
        # è®°å½•è§£ææ—¶é—´
        if hasattr(event, 'processing_time'):
            self.metrics.parse_times.append(event.processing_time)
            if len(self.metrics.parse_times) > 100:  # ä¿æŒæœ€è¿‘100æ¬¡è®°å½•
                self.metrics.parse_times.pop(0)
            
            self.metrics.avg_parse_time = sum(self.metrics.parse_times) / len(self.metrics.parse_times)
    
    async def _on_error_event(self, event):
        """é”™è¯¯äº‹ä»¶å¤„ç†"""
        self.metrics.total_events += 1
        self.metrics.error_events += 1
        print(f"âš ï¸ è§£æé”™è¯¯: {event.data.get('error', 'Unknown error')}")
    
    async def _on_complete_event(self, event):
        """å®Œæˆäº‹ä»¶å¤„ç†"""
        session_id = event.session_id
        current_data = self.parser.get_current_data(session_id)
        
        if current_data:
            # æ£€æŸ¥æ•°æ®å˜åŒ–
            if session_id in self.data_snapshots:
                old_data = self.data_snapshots[session_id]
                diff_result = self.diff_engine.compute_diff(old_data, current_data)
                
                if diff_result.changes:
                    print(f"ğŸ“Š æ•°æ®å˜åŒ–æ£€æµ‹ (ä¼šè¯: {session_id}):")
                    for change in diff_result.changes[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªå˜åŒ–
                        print(f"  {change.operation}: {change.path} = {change.new_value}")
            
            # æ›´æ–°å¿«ç…§
            self.data_snapshots[session_id] = current_data.copy()
    
    async def start_monitoring(self, data_sources: List[str]):
        """å¼€å§‹ç›‘æ§æ•°æ®æº"""
        print("ğŸš€ å¼€å§‹å®æ—¶æ•°æ®ç›‘æ§...")
        
        # ä¸ºæ¯ä¸ªæ•°æ®æºåˆ›å»ºä¼šè¯
        sessions = {}
        for source in data_sources:
            session_id = self.parser.create_session()
            sessions[source] = session_id
            print(f"ğŸ“¡ åˆ›å»ºç›‘æ§ä¼šè¯: {source} -> {session_id}")
        
        self.metrics.active_sessions = len(sessions)
        
        # æ¨¡æ‹Ÿæ•°æ®æµ
        await self._simulate_data_streams(sessions)
    
    async def _simulate_data_streams(self, sessions: Dict[str, str]):
        """æ¨¡æ‹Ÿæ•°æ®æµ"""
        # æ¨¡æ‹Ÿä¸åŒæ•°æ®æºçš„JSONæ•°æ®æµ
        data_streams = {
            "user_service": [
                '{"users": [',
                '{"id": 1, "name": "Alice", "status": "online"},',
                '{"id": 2, "name": "Bob", "status": "offline"}',
                '], "timestamp": "2024-12-01T10:30:00Z"}'
            ],
            "order_service": [
                '{"orders": [',
                '{"id": "ord_001", "amount": 99.99, "status": "pending"},',
                '{"id": "ord_002", "amount": 149.99, "status": "completed"}',
                '], "total_amount": 249.98}'
            ],
            "metrics_service": [
                '{"metrics": {',
                '"cpu_usage": 45.2,',
                '"memory_usage": 67.8,',
                '"disk_usage": 23.1',
                '}, "timestamp": "2024-12-01T10:30:00Z"}'
            ]
        }
        
        # å¹¶å‘å¤„ç†æ‰€æœ‰æ•°æ®æµ
        tasks = []
        for source, session_id in sessions.items():
            if source in data_streams:
                task = self._process_data_stream(
                    source, session_id, data_streams[source]
                )
                tasks.append(task)
        
        await asyncio.gather(*tasks)
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        await self._print_final_stats(sessions)
    
    async def _process_data_stream(self, source: str, session_id: str, chunks: List[str]):
        """å¤„ç†å•ä¸ªæ•°æ®æµ"""
        print(f"\nğŸ“ˆ å¼€å§‹å¤„ç† {source} æ•°æ®æµ...")
        
        for i, chunk in enumerate(chunks):
            start_time = time.time()
            
            # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
            await asyncio.sleep(0.1)
            
            # è§£ææ•°æ®å—
            result = await self.parser.parse_chunk(
                session_id=session_id,
                chunk=chunk
            )
            
            processing_time = time.time() - start_time
            
            # è·å–å½“å‰çŠ¶æ€
            state = self.parser.parsing_states[session_id]
            progress = (i + 1) / len(chunks)
            
            print(f"  {source}: å— {i+1}/{len(chunks)} å¤„ç†å®Œæˆ ({progress:.1%}) - {processing_time:.3f}s")
            
            # å¦‚æœæœ‰éƒ¨åˆ†æ•°æ®ï¼Œæ˜¾ç¤ºé¢„è§ˆ
            if state.current_data:
                preview = str(state.current_data)[:100]
                print(f"    é¢„è§ˆ: {preview}...")
    
    async def _print_final_stats(self, sessions: Dict[str, str]):
        """è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ“Š ç›‘æ§ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 50)
        print(f"æ€»äº‹ä»¶æ•°: {self.metrics.total_events}")
        print(f"è§£æäº‹ä»¶: {self.metrics.parse_events}")
        print(f"é”™è¯¯äº‹ä»¶: {self.metrics.error_events}")
        print(f"æ´»è·ƒä¼šè¯: {self.metrics.active_sessions}")
        print(f"å¹³å‡è§£ææ—¶é—´: {self.metrics.avg_parse_time:.3f}s")
        
        print("\nğŸ“‹ ä¼šè¯è¯¦æƒ…:")
        for source, session_id in sessions.items():
            final_data = self.parser.get_current_data(session_id)
            if final_data:
                print(f"  {source}: è§£ææˆåŠŸ - {len(str(final_data))} å­—ç¬¦")
            else:
                print(f"  {source}: è§£æå¤±è´¥")
            
            # æ¸…ç†ä¼šè¯
            self.parser.cleanup_session(session_id)
        
        print("\nâœ… ç›‘æ§å®Œæˆ")

# ä½¿ç”¨ç¤ºä¾‹
async def monitoring_example():
    monitor = RealTimeMonitor()
    
    # å®šä¹‰è¦ç›‘æ§çš„æ•°æ®æº
    data_sources = [
        "user_service",
        "order_service", 
        "metrics_service"
    ]
    
    # å¼€å§‹ç›‘æ§
    await monitor.start_monitoring(data_sources)

# è¿è¡Œç¤ºä¾‹
asyncio.run(monitoring_example())
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### æ€§èƒ½æŒ‡æ ‡

AgentlyFormat v2.0.0 åœ¨æ€§èƒ½æ–¹é¢æœ‰æ˜¾è‘—æå‡ï¼š

| æŒ‡æ ‡ | v1.0.0 | v2.0.0 | æ”¹è¿›å¹…åº¦ |
|------|--------|--------|----------|
| é€‚é…å™¨åˆ›å»ºæ—¶é—´ | 19.5s | 3.75s | **81% â¬‡ï¸** |
| æµ‹è¯•æ‰§è¡Œæ—¶é—´ | ~15s | 3.75s | **75% â¬‡ï¸** |
| å†…å­˜ä½¿ç”¨ | åŸºå‡† | -50% | **50% â¬‡ï¸** |
| APIå“åº”æ—¶é—´ | ~500ms | ~100ms | **80% â¬‡ï¸** |
| å¹¶å‘å¤„ç†èƒ½åŠ› | 10 req/s | 100 req/s | **900% â¬†ï¸** |
| é”™è¯¯ç‡ | ~5% | <0.1% | **98% â¬‡ï¸** |

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

#### 1. å†…å­˜ä¼˜åŒ–

```python
# ä½¿ç”¨ç¯å½¢ç¼“å†²åŒºå‡å°‘å†…å­˜å ç”¨
from agently_format.core.streaming_parser import StreamingParser

parser = StreamingParser(
    buffer_size=8192,  # 8KBç¼“å†²åŒº
    max_chunk_size=1024*1024,  # 1MBæœ€å¤§å—å¤§å°
    enable_compression=True  # å¯ç”¨å‹ç¼©
)

# åŠæ—¶æ¸…ç†ä¼šè¯
session_id = parser.create_session()
# ... å¤„ç†æ•°æ® ...
parser.cleanup_session(session_id)  # é‡è¦ï¼šåŠæ—¶æ¸…ç†
```

#### 2. å¹¶å‘å¤„ç†

```python
import asyncio
from agently_format.core.json_completer import JSONCompleter

async def concurrent_completion(json_list: list):
    completer = JSONCompleter()
    
    # å¹¶å‘å¤„ç†å¤šä¸ªJSONè¡¥å…¨ä»»åŠ¡
    tasks = [
        completer.complete(json_str) 
        for json_str in json_list
    ]
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return results

# ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
semaphore = asyncio.Semaphore(10)  # æœ€å¤š10ä¸ªå¹¶å‘ä»»åŠ¡

async def limited_concurrent_processing(json_list: list):
    async def process_with_limit(json_str):
        async with semaphore:
            return await completer.complete(json_str)
    
    tasks = [process_with_limit(json_str) for json_str in json_list]
    return await asyncio.gather(*tasks)
```

#### 3. ç¼“å­˜ä¼˜åŒ–

```python
from agently_format.core.json_completer import JSONCompleter
from agently_format.core.path_builder import PathBuilder

# å¯ç”¨ç¼“å­˜
completer = JSONCompleter(enable_cache=True, cache_size=1000)
builder = PathBuilder(enable_cache=True, cache_ttl=300)  # 5åˆ†é’ŸTTL

# é¢„çƒ­ç¼“å­˜
common_patterns = [
    '{"name": "',
    '{"id": ',
    '{"data": ['
]

for pattern in common_patterns:
    completer.complete(pattern)  # é¢„çƒ­ç¼“å­˜
```

#### 4. æ‰¹é‡å¤„ç†ä¼˜åŒ–

```python
from agently_format.api.batch import BatchProcessor

# ä½¿ç”¨æ‰¹é‡å¤„ç†å™¨
batch_processor = BatchProcessor(
    batch_size=100,  # æ‰¹é‡å¤§å°
    max_workers=4,   # å·¥ä½œçº¿ç¨‹æ•°
    timeout=30       # è¶…æ—¶æ—¶é—´
)

# æ‰¹é‡å¤„ç†JSONè¡¥å…¨
results = await batch_processor.process_completions(json_list)

# æ‰¹é‡è·¯å¾„æ„å»º
path_results = await batch_processor.process_path_building(data_list)
```

### ç›‘æ§å’Œè°ƒä¼˜

#### 1. æ€§èƒ½ç›‘æ§

```python
import time
import psutil
from agently_format.core.streaming_parser import StreamingParser

class PerformanceMonitor:
    def __init__(self):
        self.start_time = time.time()
        self.process = psutil.Process()
    
    def get_metrics(self):
        return {
            "uptime": time.time() - self.start_time,
            "cpu_percent": self.process.cpu_percent(),
            "memory_mb": self.process.memory_info().rss / 1024 / 1024,
            "threads": self.process.num_threads()
        }
    
    def log_metrics(self):
        metrics = self.get_metrics()
        print(f"æ€§èƒ½æŒ‡æ ‡: CPU={metrics['cpu_percent']:.1f}% "
              f"å†…å­˜={metrics['memory_mb']:.1f}MB "
              f"çº¿ç¨‹={metrics['threads']}")

# ä½¿ç”¨ç›‘æ§å™¨
monitor = PerformanceMonitor()

# å®šæœŸè¾“å‡ºæ€§èƒ½æŒ‡æ ‡
import asyncio

async def periodic_monitoring():
    while True:
        monitor.log_metrics()
        await asyncio.sleep(10)  # æ¯10ç§’è¾“å‡ºä¸€æ¬¡

# åœ¨åå°è¿è¡Œç›‘æ§
asyncio.create_task(periodic_monitoring())
```

#### 2. æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
import time
import statistics
from agently_format.core.json_completer import JSONCompleter

def benchmark_completion(test_cases: list, iterations: int = 100):
    """JSONè¡¥å…¨æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    completer = JSONCompleter()
    results = []
    
    for test_case in test_cases:
        times = []
        
        for _ in range(iterations):
            start_time = time.perf_counter()
            result = completer.complete(test_case)
            end_time = time.perf_counter()
            
            if result.is_valid:
                times.append(end_time - start_time)
        
        if times:
            results.append({
                "test_case": test_case[:50] + "...",
                "avg_time": statistics.mean(times),
                "min_time": min(times),
                "max_time": max(times),
                "std_dev": statistics.stdev(times) if len(times) > 1 else 0,
                "success_rate": len(times) / iterations
            })
    
    return results

# è¿è¡ŒåŸºå‡†æµ‹è¯•
test_cases = [
    '{"simple": "test"',
    '{"nested": {"data": [1, 2, 3',
    '{"complex": {"users": [{"name": "Alice", "profile": {"age": 25'
]

benchmark_results = benchmark_completion(test_cases)

for result in benchmark_results:
    print(f"æµ‹è¯•ç”¨ä¾‹: {result['test_case']}")
    print(f"  å¹³å‡æ—¶é—´: {result['avg_time']*1000:.2f}ms")
    print(f"  æˆåŠŸç‡: {result['success_rate']:.1%}")
    print()
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. JSONè¡¥å…¨å¤±è´¥

**é—®é¢˜**: JSONè¡¥å…¨è¿”å›æ— æ•ˆç»“æœ

**å¯èƒ½åŸå› **:
- è¾“å…¥JSONæ ¼å¼ä¸¥é‡æŸå
- è¡¥å…¨ç­–ç•¥ä¸é€‚åˆå½“å‰æ•°æ®
- å†…å­˜ä¸è¶³å¯¼è‡´å¤„ç†å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```python
from agently_format.core.json_completer import JSONCompleter, CompletionStrategy

completer = JSONCompleter()

# å°è¯•ä¸åŒçš„è¡¥å…¨ç­–ç•¥
strategies = [CompletionStrategy.SMART, CompletionStrategy.CONSERVATIVE, CompletionStrategy.AGGRESSIVE]

for strategy in strategies:
    result = completer.complete(incomplete_json, strategy=strategy)
    if result.is_valid:
        print(f"ä½¿ç”¨ {strategy} ç­–ç•¥è¡¥å…¨æˆåŠŸ")
        break
else:
    print("æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œæ£€æŸ¥è¾“å…¥æ•°æ®")
    
    # è¯Šæ–­è¾“å…¥æ•°æ®
    diagnostic = completer.diagnose(incomplete_json)
    print(f"è¯Šæ–­ç»“æœ: {diagnostic}")
```

#### 2. æµå¼è§£æå¡ä½

**é—®é¢˜**: æµå¼è§£æå™¨åœæ­¢å“åº”

**å¯èƒ½åŸå› **:
- æ•°æ®å—è¿‡å¤§å¯¼è‡´å†…å­˜æº¢å‡º
- ä¼šè¯è¶…æ—¶
- äº‹ä»¶å¾ªç¯é˜»å¡

**è§£å†³æ–¹æ¡ˆ**:
```python
import asyncio
from agently_format.core.streaming_parser import StreamingParser

# è®¾ç½®åˆç†çš„å‚æ•°
parser = StreamingParser(
    max_chunk_size=1024*512,  # 512KB
    session_ttl=1800,         # 30åˆ†é’Ÿ
    max_sessions=100          # é™åˆ¶ä¼šè¯æ•°
)

# æ·»åŠ è¶…æ—¶å¤„ç†
async def parse_with_timeout(session_id, chunk, timeout=10):
    try:
        result = await asyncio.wait_for(
            parser.parse_chunk(session_id, chunk),
            timeout=timeout
        )
        return result
    except asyncio.TimeoutError:
        print(f"è§£æè¶…æ—¶ï¼Œä¼šè¯: {session_id}")
        parser.cleanup_session(session_id)
        return None

# ç›‘æ§ä¼šè¯çŠ¶æ€
def check_session_health():
    stats = parser.get_stats()
    if stats.get('active_sessions', 0) > 50:
        print("è­¦å‘Š: æ´»è·ƒä¼šè¯è¿‡å¤šï¼Œè€ƒè™‘æ¸…ç†")
        # æ¸…ç†è¶…æ—¶ä¼šè¯
        parser.cleanup_expired_sessions()
```

#### 3. APIå“åº”æ…¢

**é—®é¢˜**: APIæ¥å£å“åº”æ—¶é—´è¿‡é•¿

**å¯èƒ½åŸå› **:
- å¹¶å‘è¯·æ±‚è¿‡å¤š
- æ•°æ®å¤„ç†é‡å¤§
- èµ„æºç«äº‰

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. å¯ç”¨è¯·æ±‚é™æµ
from agently_format.api.middleware import RateLimitMiddleware

app.add_middleware(
    RateLimitMiddleware,
    calls=100,
    period=60  # æ¯åˆ†é’Ÿ100æ¬¡è¯·æ±‚
)

# 2. ä½¿ç”¨è¿æ¥æ± 
import httpx

client = httpx.AsyncClient(
    limits=httpx.Limits(
        max_keepalive_connections=20,
        max_connections=100
    ),
    timeout=30.0
)

# 3. å¯ç”¨å“åº”ç¼“å­˜
from agently_format.api.middleware import CacheMiddleware

app.add_middleware(
    CacheMiddleware,
    ttl=300,  # 5åˆ†é’Ÿç¼“å­˜
    max_size=1000
)
```

#### 4. å†…å­˜æ³„æ¼

**é—®é¢˜**: é•¿æ—¶é—´è¿è¡Œåå†…å­˜æŒç»­å¢é•¿

**å¯èƒ½åŸå› **:
- ä¼šè¯æœªåŠæ—¶æ¸…ç†
- äº‹ä»¶ç›‘å¬å™¨æœªæ­£ç¡®ç§»é™¤
- ç¼“å­˜æ— é™å¢é•¿

**è§£å†³æ–¹æ¡ˆ**:
```python
import gc
import weakref
from agently_format.core.streaming_parser import StreamingParser

class MemoryManagedParser:
    def __init__(self):
        self.parser = StreamingParser()
        self.session_refs = weakref.WeakSet()
    
    def create_session(self):
        session_id = self.parser.create_session()
        self.session_refs.add(session_id)
        return session_id
    
    def cleanup_all_sessions(self):
        """æ¸…ç†æ‰€æœ‰ä¼šè¯"""
        for session_id in list(self.session_refs):
            self.parser.cleanup_session(session_id)
        gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
    
    def get_memory_usage(self):
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        import psutil
        process = psutil.Process()
        return {
            "rss_mb": process.memory_info().rss / 1024 / 1024,
            "active_sessions": len(self.session_refs)
        }

# å®šæœŸæ¸…ç†å†…å­˜
async def memory_cleanup_task(parser):
    while True:
        await asyncio.sleep(300)  # æ¯5åˆ†é’Ÿ
        memory_info = parser.get_memory_usage()
        
        if memory_info["rss_mb"] > 500:  # è¶…è¿‡500MB
            print("å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œæ‰§è¡Œæ¸…ç†")
            parser.cleanup_all_sessions()
```

### è°ƒè¯•å·¥å…·

#### 1. æ—¥å¿—é…ç½®

```python
import logging
from agently_format.core.logging import setup_logging

# é…ç½®è¯¦ç»†æ—¥å¿—
setup_logging(
    level=logging.DEBUG,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("agently_format.log")
    ]
)

# è·å–ç‰¹å®šæ¨¡å—çš„æ—¥å¿—å™¨
parser_logger = logging.getLogger("agently_format.core.streaming_parser")
completer_logger = logging.getLogger("agently_format.core.json_completer")

# è®¾ç½®ä¸åŒçº§åˆ«
parser_logger.setLevel(logging.INFO)
completer_logger.setLevel(logging.DEBUG)
```

#### 2. æ€§èƒ½åˆ†æ

```python
import cProfile
import pstats
from agently_format.core.json_completer import JSONCompleter

def profile_completion(json_str: str):
    """æ€§èƒ½åˆ†æJSONè¡¥å…¨"""
    completer = JSONCompleter()
    
    # åˆ›å»ºæ€§èƒ½åˆ†æå™¨
    profiler = cProfile.Profile()
    
    # å¼€å§‹åˆ†æ
    profiler.enable()
    result = completer.complete(json_str)
    profiler.disable()
    
    # è¾“å‡ºåˆ†æç»“æœ
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(10)  # æ˜¾ç¤ºå‰10ä¸ªæœ€è€—æ—¶çš„å‡½æ•°
    
    return result

# ä½¿ç”¨ç¤ºä¾‹
result = profile_completion('{"large": "json", "data": [')
```

#### 3. å†…å­˜åˆ†æ

```python
import tracemalloc
from agently_format.core.streaming_parser import StreamingParser

async def memory_trace_parsing():
    """å†…å­˜è¿½è¸ªè§£æè¿‡ç¨‹"""
    # å¼€å§‹å†…å­˜è¿½è¸ª
    tracemalloc.start()
    
    parser = StreamingParser()
    session_id = parser.create_session()
    
    # è®°å½•åˆå§‹å†…å­˜
    snapshot1 = tracemalloc.take_snapshot()
    
    # æ‰§è¡Œè§£ææ“ä½œ
    chunks = ['{"data": [' + str(i) + ',' for i in range(1000)]
    for chunk in chunks:
        await parser.parse_chunk(session_id, chunk)
    
    # è®°å½•ç»“æŸå†…å­˜
    snapshot2 = tracemalloc.take_snapshot()
    
    # åˆ†æå†…å­˜å·®å¼‚
    top_stats = snapshot2.compare_to(snapshot1, 'lineno')
    
    print("å†…å­˜ä½¿ç”¨Top 10:")
    for stat in top_stats[:10]:
        print(stat)
    
    # æ¸…ç†
    parser.cleanup_session(session_id)
    tracemalloc.stop()
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. æ¶æ„è®¾è®¡åŸåˆ™

#### å•ä¸€èŒè´£åŸåˆ™
```python
# âœ… å¥½çš„è®¾è®¡ - æ¯ä¸ªç±»èŒè´£å•ä¸€
class JSONCompleter:
    """ä¸“é—¨è´Ÿè´£JSONè¡¥å…¨"""
    def complete(self, json_str: str) -> CompletionResult:
        pass

class StreamingParser:
    """ä¸“é—¨è´Ÿè´£æµå¼è§£æ"""
    async def parse_chunk(self, session_id: str, chunk: str) -> ParseResult:
        pass

class PathBuilder:
    """ä¸“é—¨è´Ÿè´£è·¯å¾„æ„å»º"""
    def extract_parsing_key_orders(self, data: dict) -> List[str]:
        pass

# âŒ ä¸å¥½çš„è®¾è®¡ - èŒè´£æ··ä¹±
class JSONProcessor:
    """ä»€ä¹ˆéƒ½åšçš„ç±»"""
    def complete_and_parse_and_build_paths(self, json_str: str):
        pass  # èŒè´£è¿‡å¤š
```

#### ä¾èµ–æ³¨å…¥
```python
from typing import Protocol

class EventEmitterProtocol(Protocol):
    async def emit(self, event) -> None:
        pass

class StreamingParser:
    def __init__(self, event_emitter: EventEmitterProtocol):
        self.event_emitter = event_emitter  # ä¾èµ–æ³¨å…¥
    
    async def parse_chunk(self, session_id: str, chunk: str):
        # ä½¿ç”¨æ³¨å…¥çš„ä¾èµ–
        await self.event_emitter.emit(parse_event)
```

### 2. é”™è¯¯å¤„ç†ç­–ç•¥

#### åˆ†å±‚é”™è¯¯å¤„ç†
```python
from typing import Union, Optional
from dataclasses import dataclass

@dataclass
class ProcessingError:
    code: str
    message: str
    details: Optional[dict] = None
    recoverable: bool = True

class JSONCompleter:
    def complete(self, json_str: str) -> Union[CompletionResult, ProcessingError]:
        try:
            # æ ¸å¿ƒå¤„ç†é€»è¾‘
            return self._do_complete(json_str)
        except ValueError as e:
            return ProcessingError(
                code="INVALID_INPUT",
                message=f"è¾“å…¥JSONæ ¼å¼æ— æ•ˆ: {e}",
                recoverable=False
            )
        except MemoryError as e:
            return ProcessingError(
                code="MEMORY_EXHAUSTED",
                message="å†…å­˜ä¸è¶³ï¼Œè¯·å‡å°‘æ•°æ®é‡",
                recoverable=True
            )
        except Exception as e:
            return ProcessingError(
                code="UNKNOWN_ERROR",
                message=f"æœªçŸ¥é”™è¯¯: {e}",
                details={"exception_type": type(e).__name__},
                recoverable=False
            )
```

#### é‡è¯•æœºåˆ¶
```python
import asyncio
from functools import wraps

def retry_on_failure(max_retries: int = 3, delay: float = 1.0):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_retries + 1):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_retries:
                        await asyncio.sleep(delay * (2 ** attempt))  # æŒ‡æ•°é€€é¿
                        continue
                    break
            
            raise last_exception
        return wrapper
    return decorator

class ModelAdapter:
    @retry_on_failure(max_retries=3, delay=1.0)
    async def chat_completion(self, messages: list) -> ChatResponse:
        # å¯èƒ½å¤±è´¥çš„ç½‘ç»œè¯·æ±‚
        pass
```

### 3. èµ„æºç®¡ç†

#### ä¸Šä¸‹æ–‡ç®¡ç†å™¨
```python
from contextlib import asynccontextmanager
from typing import AsyncGenerator

class StreamingParser:
    @asynccontextmanager
    async def session_context(self) -> AsyncGenerator[str, None]:
        """ä¼šè¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        session_id = self.create_session()
        try:
            yield session_id
        finally:
            self.cleanup_session(session_id)

# ä½¿ç”¨ç¤ºä¾‹
async def process_with_session():
    parser = StreamingParser()
    
    async with parser.session_context() as session_id:
        # è‡ªåŠ¨ç®¡ç†ä¼šè¯ç”Ÿå‘½å‘¨æœŸ
        result = await parser.parse_chunk(session_id, chunk)
        return result
    # ä¼šè¯è‡ªåŠ¨æ¸…ç†
```

#### è¿æ¥æ± ç®¡ç†
```python
import asyncio
from typing import Dict, Optional

class ConnectionPool:
    def __init__(self, max_connections: int = 10):
        self.max_connections = max_connections
        self.connections: Dict[str, object] = {}
        self.semaphore = asyncio.Semaphore(max_connections)
    
    async def get_connection(self, key: str) -> object:
        async with self.semaphore:
            if key not in self.connections:
                self.connections[key] = await self._create_connection(key)
            return self.connections[key]
    
    async def _create_connection(self, key: str) -> object:
        # åˆ›å»ºå®é™…è¿æ¥
        pass
    
    async def close_all(self):
        for conn in self.connections.values():
            await self._close_connection(conn)
        self.connections.clear()
```

### 4. æµ‹è¯•ç­–ç•¥

#### å•å…ƒæµ‹è¯•
```python
import pytest
from unittest.mock import Mock, AsyncMock
from agently_format.core.json_completer import JSONCompleter

class TestJSONCompleter:
    def setup_method(self):
        self.completer = JSONCompleter()
    
    def test_simple_completion(self):
        """æµ‹è¯•ç®€å•JSONè¡¥å…¨"""
        incomplete = '{"name": "Alice"'
        result = self.completer.complete(incomplete)
        
        assert result.is_valid
        assert result.completed_json == '{"name": "Alice"}'
    
    def test_complex_nested_completion(self):
        """æµ‹è¯•å¤æ‚åµŒå¥—JSONè¡¥å…¨"""
        incomplete = '{"user": {"profile": {"age": 25'
        result = self.completer.complete(incomplete)
        
        assert result.is_valid
        assert '}}' in result.completed_json
    
    def test_invalid_input_handling(self):
        """æµ‹è¯•æ— æ•ˆè¾“å…¥å¤„ç†"""
        invalid_input = "not json at all"
        result = self.completer.complete(invalid_input)
        
        assert not result.is_valid
        assert result.error_message is not None
    
    @pytest.mark.parametrize("strategy", [
        "smart", "conservative", "aggressive"
    ])
    def test_different_strategies(self, strategy):
        """æµ‹è¯•ä¸åŒè¡¥å…¨ç­–ç•¥"""
        incomplete = '{"data": [1, 2'
        result = self.completer.complete(incomplete, strategy=strategy)
        
        # æ‰€æœ‰ç­–ç•¥éƒ½åº”è¯¥èƒ½å¤„ç†è¿™ä¸ªç®€å•æ¡ˆä¾‹
        assert result.is_valid
```

#### é›†æˆæµ‹è¯•
```python
import pytest
import asyncio
from agently_format.core.streaming_parser import StreamingParser
from agently_format.core.event_system import EventEmitter

@pytest.mark.asyncio
class TestStreamingIntegration:
    async def test_end_to_end_parsing(self):
        """ç«¯åˆ°ç«¯æµå¼è§£ææµ‹è¯•"""
        emitter = EventEmitter()
        parser = StreamingParser(emitter)
        
        # æ”¶é›†äº‹ä»¶
        events = []
        async def event_collector(event):
            events.append(event)
        
        emitter.on('parse_progress', event_collector)
        
        # æ‰§è¡Œè§£æ
        session_id = parser.create_session()
        chunks = ['{"users": [', '{"id": 1}', ']}']
        
        for chunk in chunks:
            await parser.parse_chunk(session_id, chunk)
        
        # éªŒè¯ç»“æœ
        final_data = parser.get_current_data(session_id)
        assert final_data == {"users": [{"id": 1}]}
        assert len(events) > 0  # ç¡®ä¿äº‹ä»¶è¢«è§¦å‘
        
        parser.cleanup_session(session_id)
```

#### æ€§èƒ½æµ‹è¯•
```python
import time
import pytest
from agently_format.core.json_completer import JSONCompleter

class TestPerformance:
    def test_completion_performance(self):
        """æµ‹è¯•è¡¥å…¨æ€§èƒ½"""
        completer = JSONCompleter()
        large_json = '{"data": [' + ','.join([f'{{"id": {i}}}' for i in range(1000)])
        
        start_time = time.perf_counter()
        result = completer.complete(large_json)
        end_time = time.perf_counter()
        
        processing_time = end_time - start_time
        
        assert result.is_valid
        assert processing_time < 1.0  # åº”è¯¥åœ¨1ç§’å†…å®Œæˆ
    
    @pytest.mark.benchmark
    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss
        
        completer = JSONCompleter()
        
        # å¤„ç†å¤§é‡æ•°æ®
        for i in range(100):
            large_json = '{"batch": ' + str(i) + ', "data": ['
            completer.complete(large_json)
        
        final_memory = process.memory_info().rss
        memory_increase = final_memory - initial_memory
        
        # å†…å­˜å¢é•¿åº”è¯¥åœ¨åˆç†èŒƒå›´å†…ï¼ˆæ¯”å¦‚50MBï¼‰
        assert memory_increase < 50 * 1024 * 1024
```

### 5. éƒ¨ç½²å»ºè®®

#### Dockeréƒ¨ç½²
```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# å®‰è£…åº”ç”¨
RUN pip install -e .

# åˆ›å»ºérootç”¨æˆ·
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "agently_format.api.app:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### Docker Compose
```yaml
# docker-compose.yml
version: '3.8'

services:
  agently-format:
    build: .
    ports:
      - "8000:8000"
    environment:
      - AGENTLY_FORMAT_DEBUG=false
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
  
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - agently-format
    restart: unless-stopped
```

#### ç”Ÿäº§ç¯å¢ƒé…ç½®
```python
# production_config.py
from agently_format.api.config import Settings

class ProductionSettings(Settings):
    debug: bool = False
    log_level: str = "INFO"
    
    # æ€§èƒ½ä¼˜åŒ–
    max_workers: int = 4
    max_chunk_size: int = 1024 * 1024  # 1MB
    session_ttl: int = 1800  # 30åˆ†é’Ÿ
    
    # å®‰å…¨é…ç½®
    rate_limit_enabled: bool = True
    rate_limit_requests: int = 1000
    rate_limit_window: int = 60
    
    # ç›‘æ§é…ç½®
    metrics_enabled: bool = True
    health_check_enabled: bool = True
    
    class Config:
        env_file = ".env.production"
```

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### è·å–å¸®åŠ©

- **GitHub Issues**: [https://github.com/AgentEra/AgentlyFormat/issues](https://github.com/AgentEra/AgentlyFormat/issues)
- **æ–‡æ¡£**: [https://AgentlyFormat.readthedocs.io](https://AgentlyFormat.readthedocs.io)
- **é‚®ç®±æ”¯æŒ**: support@agently.tech

### è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/amazing-feature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add amazing feature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/amazing-feature`)
5. åˆ›å»º Pull Request

### ç‰ˆæœ¬è§„åˆ’

- **v2.1.0**: æ›´å¤šæ¨¡å‹æ”¯æŒã€æ’ä»¶ç³»ç»Ÿ
- **v2.2.0**: Webç®¡ç†ç•Œé¢ã€é›†ç¾¤æ”¯æŒ
- **v3.0.0**: æœºå™¨å­¦ä¹ é›†æˆã€å¤šè¯­è¨€æ”¯æŒ

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [Apache-2.0](https://opensource.org/licenses/Apache-2.0) è®¸å¯è¯ã€‚

---

**AgentlyFormat** - è®©å¤§æ¨¡å‹JSONè¾“å‡ºæ›´ç¨³å®šã€æ›´å¯é ï¼ ğŸš€

*æœ€åæ›´æ–°: 2024å¹´12æœˆ*